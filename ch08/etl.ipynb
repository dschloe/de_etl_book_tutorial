{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e590d45d-1ad5-480b-ab75-404e4035d35a",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69530bed-7b76-43ec-8d23-c94db601710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE] http://books.toscrape.com/catalogue/page-1.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-2.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-3.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-4.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-5.html\n",
      "[INFO] 수집된 책 URL 수: 100\n",
      "[PROGRESS] 1/100 완료 (1.0s 경과, ETA 96.7s)\n",
      "[PROGRESS] 2/100 완료 (1.1s 경과, ETA 51.6s)\n",
      "[PROGRESS] 3/100 완료 (1.3s 경과, ETA 41.2s)\n",
      "[PROGRESS] 4/100 완료 (1.7s 경과, ETA 40.0s)\n",
      "[PROGRESS] 5/100 완료 (1.9s 경과, ETA 35.4s)\n",
      "[PROGRESS] 6/100 완료 (2.0s 경과, ETA 31.8s)\n",
      "[PROGRESS] 7/100 완료 (2.0s 경과, ETA 26.9s)\n",
      "[PROGRESS] 8/100 완료 (2.2s 경과, ETA 25.2s)\n",
      "[PROGRESS] 9/100 완료 (2.2s 경과, ETA 22.2s)\n",
      "[PROGRESS] 10/100 완료 (2.3s 경과, ETA 20.8s)\n",
      "[PROGRESS] 11/100 완료 (2.4s 경과, ETA 19.0s)\n",
      "[PROGRESS] 12/100 완료 (2.4s 경과, ETA 17.3s)\n",
      "[PROGRESS] 13/100 완료 (2.4s 경과, ETA 16.0s)\n",
      "[PROGRESS] 14/100 완료 (2.5s 경과, ETA 15.1s)\n",
      "[PROGRESS] 15/100 완료 (2.5s 경과, ETA 14.1s)\n",
      "[PROGRESS] 16/100 완료 (2.6s 경과, ETA 13.5s)\n",
      "[PROGRESS] 17/100 완료 (2.8s 경과, ETA 13.8s)\n",
      "[PROGRESS] 18/100 완료 (2.8s 경과, ETA 12.9s)\n",
      "[PROGRESS] 19/100 완료 (2.8s 경과, ETA 12.1s)\n",
      "[PROGRESS] 20/100 완료 (3.0s 경과, ETA 11.9s)\n",
      "[PROGRESS] 21/100 완료 (3.0s 경과, ETA 11.3s)\n",
      "[PROGRESS] 22/100 완료 (3.0s 경과, ETA 10.7s)\n",
      "[PROGRESS] 23/100 완료 (3.2s 경과, ETA 10.7s)\n",
      "[PROGRESS] 24/100 완료 (3.4s 경과, ETA 10.7s)\n",
      "[PROGRESS] 25/100 완료 (3.4s 경과, ETA 10.2s)\n",
      "[PROGRESS] 26/100 완료 (3.5s 경과, ETA 9.9s)\n",
      "[PROGRESS] 27/100 완료 (3.5s 경과, ETA 9.5s)\n",
      "[PROGRESS] 28/100 완료 (3.5s 경과, ETA 9.1s)\n",
      "[PROGRESS] 29/100 완료 (3.7s 경과, ETA 9.1s)\n",
      "[PROGRESS] 30/100 완료 (3.8s 경과, ETA 8.8s)\n",
      "[PROGRESS] 31/100 완료 (3.8s 경과, ETA 8.5s)\n",
      "[PROGRESS] 32/100 완료 (3.8s 경과, ETA 8.1s)\n",
      "[PROGRESS] 33/100 완료 (4.0s 경과, ETA 8.1s)\n",
      "[PROGRESS] 34/100 완료 (4.1s 경과, ETA 7.9s)\n",
      "[PROGRESS] 35/100 완료 (4.1s 경과, ETA 7.6s)\n",
      "[PROGRESS] 36/100 완료 (4.1s 경과, ETA 7.4s)\n",
      "[PROGRESS] 37/100 완료 (4.4s 경과, ETA 7.5s)\n",
      "[PROGRESS] 38/100 완료 (4.4s 경과, ETA 7.2s)\n",
      "[PROGRESS] 39/100 완료 (4.7s 경과, ETA 7.4s)\n",
      "[PROGRESS] 40/100 완료 (4.7s 경과, ETA 7.1s)\n",
      "[PROGRESS] 41/100 완료 (4.7s 경과, ETA 6.8s)\n",
      "[PROGRESS] 42/100 완료 (4.8s 경과, ETA 6.7s)\n",
      "[PROGRESS] 43/100 완료 (4.9s 경과, ETA 6.5s)\n",
      "[PROGRESS] 44/100 완료 (4.9s 경과, ETA 6.2s)\n",
      "[PROGRESS] 45/100 완료 (5.2s 경과, ETA 6.3s)\n",
      "[PROGRESS] 46/100 완료 (5.2s 경과, ETA 6.1s)\n",
      "[PROGRESS] 47/100 완료 (5.2s 경과, ETA 5.9s)\n",
      "[PROGRESS] 48/100 완료 (5.3s 경과, ETA 5.7s)\n",
      "[PROGRESS] 49/100 완료 (5.3s 경과, ETA 5.5s)\n",
      "[PROGRESS] 50/100 완료 (5.4s 경과, ETA 5.4s)\n",
      "[PROGRESS] 51/100 완료 (5.4s 경과, ETA 5.2s)\n",
      "[PROGRESS] 52/100 완료 (5.5s 경과, ETA 5.1s)\n",
      "[PROGRESS] 53/100 완료 (5.6s 경과, ETA 5.0s)\n",
      "[PROGRESS] 54/100 완료 (5.8s 경과, ETA 5.0s)\n",
      "[PROGRESS] 55/100 완료 (5.9s 경과, ETA 4.8s)\n",
      "[PROGRESS] 56/100 완료 (6.1s 경과, ETA 4.8s)\n",
      "[PROGRESS] 57/100 완료 (6.1s 경과, ETA 4.6s)\n",
      "[PROGRESS] 58/100 완료 (6.1s 경과, ETA 4.4s)\n",
      "[PROGRESS] 59/100 완료 (6.1s 경과, ETA 4.2s)\n",
      "[PROGRESS] 60/100 완료 (6.2s 경과, ETA 4.1s)\n",
      "[PROGRESS] 61/100 완료 (6.3s 경과, ETA 4.0s)\n",
      "[PROGRESS] 62/100 완료 (6.3s 경과, ETA 3.9s)\n",
      "[PROGRESS] 63/100 완료 (6.3s 경과, ETA 3.7s)\n",
      "[PROGRESS] 64/100 완료 (6.5s 경과, ETA 3.6s)\n",
      "[PROGRESS] 65/100 완료 (6.5s 경과, ETA 3.5s)\n",
      "[PROGRESS] 66/100 완료 (6.7s 경과, ETA 3.4s)\n",
      "[PROGRESS] 67/100 완료 (6.7s 경과, ETA 3.3s)\n",
      "[PROGRESS] 68/100 완료 (6.9s 경과, ETA 3.3s)\n",
      "[PROGRESS] 69/100 완료 (6.9s 경과, ETA 3.1s)\n",
      "[PROGRESS] 70/100 완료 (7.0s 경과, ETA 3.0s)\n",
      "[PROGRESS] 71/100 완료 (7.1s 경과, ETA 2.9s)\n",
      "[PROGRESS] 72/100 완료 (7.2s 경과, ETA 2.8s)\n",
      "[PROGRESS] 73/100 완료 (7.2s 경과, ETA 2.7s)\n",
      "[PROGRESS] 74/100 완료 (7.4s 경과, ETA 2.6s)\n",
      "[PROGRESS] 75/100 완료 (7.4s 경과, ETA 2.5s)\n",
      "[PROGRESS] 76/100 완료 (7.4s 경과, ETA 2.3s)\n",
      "[PROGRESS] 77/100 완료 (7.4s 경과, ETA 2.2s)\n",
      "[PROGRESS] 78/100 완료 (7.5s 경과, ETA 2.1s)\n",
      "[PROGRESS] 79/100 완료 (7.5s 경과, ETA 2.0s)\n",
      "[PROGRESS] 80/100 완료 (7.6s 경과, ETA 1.9s)\n",
      "[PROGRESS] 81/100 완료 (7.7s 경과, ETA 1.8s)\n",
      "[PROGRESS] 82/100 완료 (7.8s 경과, ETA 1.7s)\n",
      "[PROGRESS] 83/100 완료 (7.8s 경과, ETA 1.6s)\n",
      "[PROGRESS] 84/100 완료 (8.0s 경과, ETA 1.5s)\n",
      "[PROGRESS] 85/100 완료 (8.0s 경과, ETA 1.4s)\n",
      "[PROGRESS] 86/100 완료 (8.1s 경과, ETA 1.3s)\n",
      "[PROGRESS] 87/100 완료 (8.1s 경과, ETA 1.2s)\n",
      "[PROGRESS] 88/100 완료 (8.2s 경과, ETA 1.1s)\n",
      "[PROGRESS] 89/100 완료 (8.2s 경과, ETA 1.0s)\n",
      "[PROGRESS] 90/100 완료 (8.4s 경과, ETA 0.9s)\n",
      "[PROGRESS] 91/100 완료 (8.5s 경과, ETA 0.8s)\n",
      "[PROGRESS] 92/100 완료 (8.5s 경과, ETA 0.7s)\n",
      "[PROGRESS] 93/100 완료 (8.6s 경과, ETA 0.6s)\n",
      "[PROGRESS] 94/100 완료 (8.6s 경과, ETA 0.5s)\n",
      "[PROGRESS] 95/100 완료 (8.6s 경과, ETA 0.5s)\n",
      "[PROGRESS] 96/100 완료 (8.7s 경과, ETA 0.4s)\n",
      "[PROGRESS] 97/100 완료 (8.8s 경과, ETA 0.3s)\n",
      "[PROGRESS] 98/100 완료 (8.8s 경과, ETA 0.2s)\n",
      "[PROGRESS] 99/100 완료 (8.9s 경과, ETA 0.1s)\n",
      "[PROGRESS] 100/100 완료 (9.1s 경과, ETA 0.0s)\n",
      "Data saved to data/books_5pages.json\n",
      "Images saved to data/images/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Books to Scrape 크롤러 (멀티스레딩 / 5페이지 / 진행상황+ETA / 이미지 다운로드)\n",
    "- 상세 파싱과 동시에 표지 이미지를 data/images/ 에 저장\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "START_URL = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "MAX_WORKERS = 16\n",
    "PER_REQUEST_PAUSE = 0.03\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "print_lock = threading.Lock()\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3, connect=3, read=3, backoff_factor=0.5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\"])\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def clean_money(s):\n",
    "    return (s or \"\").replace(\"Â\", \"\").strip()\n",
    "\n",
    "def slugify(text, maxlen=80):\n",
    "    text = re.sub(r\"[^\\w\\s-]\", \"\", text, flags=re.UNICODE)  # 특수문자 제거\n",
    "    text = re.sub(r\"[\\s_-]+\", \"-\", text).strip(\"-\")         # 공백/언더스코어 → 하이픈\n",
    "    return text[:maxlen] if text else \"untitled\"\n",
    "\n",
    "def guess_ext_from_url(url, default=\"jpg\"):\n",
    "    path = urlparse(url).path\n",
    "    if \".\" in path:\n",
    "        ext = path.rsplit(\".\", 1)[-1].lower()\n",
    "        # 간단 화이트리스트\n",
    "        if ext in {\"jpg\", \"jpeg\", \"png\", \"webp\"}:\n",
    "            return \"jpg\" if ext == \"jpeg\" else ext\n",
    "    return default\n",
    "\n",
    "def download_image(sess, img_url, title, upc):\n",
    "    IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ext = guess_ext_from_url(img_url, default=\"jpg\")\n",
    "    name_part = (upc or slugify(title))\n",
    "    filename = f\"{name_part}.{ext}\"\n",
    "    dest = IMAGES_DIR / filename\n",
    "\n",
    "    # 동일 파일 존재 시 중복 방지(숫자 suffix)\n",
    "    if dest.exists():\n",
    "        i = 2\n",
    "        while True:\n",
    "            cand = IMAGES_DIR / f\"{name_part}-{i}.{ext}\"\n",
    "            if not cand.exists():\n",
    "                dest = cand\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    try:\n",
    "        with sess.get(img_url, timeout=20, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(dest, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        return str(dest)\n",
    "    except Exception as e:\n",
    "        with print_lock:\n",
    "            print(f\"(이미지 실패) {img_url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_book_detail(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    details = {}\n",
    "    table = soup.find(\"table\", class_=\"table table-striped\")\n",
    "    if table:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            th = row.find(\"th\").get_text(strip=True)\n",
    "            td = row.find(\"td\").get_text(strip=True)\n",
    "            details[th] = td\n",
    "\n",
    "    title = soup.find(\"div\", class_=\"product_main\").find(\"h1\").get_text(strip=True)\n",
    "    desc_anchor = soup.find(\"div\", id=\"product_description\")\n",
    "    description = desc_anchor.find_next_sibling(\"p\").get_text(strip=True) if desc_anchor else \"\"\n",
    "\n",
    "    img_tag = soup.select_one(\".item.active img\") or soup.find(\"img\")\n",
    "    img_rel = img_tag[\"src\"] if img_tag else \"\"\n",
    "    image_url = urljoin(base_url, img_rel)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"upc\": details.get(\"UPC\"),\n",
    "        \"product_type\": details.get(\"Product Type\", \"\"),\n",
    "        \"price_excl_tax\": clean_money(details.get(\"Price (excl. tax)\")),\n",
    "        \"price_incl_tax\": clean_money(details.get(\"Price (incl. tax)\")),\n",
    "        \"tax\": clean_money(details.get(\"Tax\")),\n",
    "        \"availability\": details.get(\"Availability\", \"\"),\n",
    "        \"num_reviews\": details.get(\"Number of reviews\"),\n",
    "        \"description\": description,\n",
    "        \"image_url\": image_url,\n",
    "        \"url\": base_url,\n",
    "    }\n",
    "\n",
    "def get_book_details(url):\n",
    "    sess = make_session()\n",
    "    try:\n",
    "        resp = sess.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = parse_book_detail(resp.text, url)\n",
    "\n",
    "        # 이미지 다운로드\n",
    "        img_saved_path = None\n",
    "        if data.get(\"image_url\"):\n",
    "            img_saved_path = download_image(\n",
    "                sess, data[\"image_url\"], data.get(\"title\", \"\"), data.get(\"upc\")\n",
    "            )\n",
    "        data[\"image_path\"] = img_saved_path\n",
    "\n",
    "        time.sleep(PER_REQUEST_PAUSE)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        with print_lock:\n",
    "            print(f\"(건너뜀) {url} -> {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        sess.close()\n",
    "\n",
    "def collect_book_urls(start_url, max_pages=5):\n",
    "    urls = []\n",
    "    page_url = start_url\n",
    "    page_count = 0\n",
    "    sess = make_session()\n",
    "\n",
    "    while page_url and page_count < max_pages:\n",
    "        with print_lock:\n",
    "            print(f\"[PAGE] {page_url}\")\n",
    "        resp = sess.get(page_url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        for art in soup.find_all(\"article\", class_=\"product_pod\"):\n",
    "            a = art.find(\"h3\").find(\"a\")\n",
    "            book_rel = a[\"href\"]\n",
    "            book_url = urljoin(page_url, book_rel)\n",
    "            urls.append(book_url)\n",
    "\n",
    "        page_count += 1\n",
    "        next_li = soup.find(\"li\", class_=\"next\")\n",
    "        page_url = urljoin(page_url, next_li.find(\"a\")[\"href\"]) if (next_li and page_count < max_pages) else None\n",
    "        time.sleep(PER_REQUEST_PAUSE)\n",
    "\n",
    "    sess.close()\n",
    "    return urls\n",
    "\n",
    "def parse_details_multithread(book_urls, max_workers=MAX_WORKERS):\n",
    "    results = []\n",
    "    total = len(book_urls)\n",
    "    done_cnt = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        fut_map = {ex.submit(get_book_details, u): u for u in book_urls}\n",
    "        for fut in as_completed(fut_map):\n",
    "            data = fut.result()\n",
    "            if data:\n",
    "                results.append(data)\n",
    "\n",
    "            done_cnt += 1\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            avg_time = elapsed / max(1, done_cnt)\n",
    "            eta = avg_time * (total - done_cnt)\n",
    "\n",
    "            with print_lock:\n",
    "                print(f\"[PROGRESS] {done_cnt}/{total} 완료 \"\n",
    "                      f\"({elapsed:.1f}s 경과, ETA {eta:.1f}s)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) URL 수집 (5페이지 한정)\n",
    "    book_urls = collect_book_urls(START_URL, max_pages=5)\n",
    "    print(f\"[INFO] 수집된 책 URL 수: {len(book_urls)}\")\n",
    "\n",
    "    # 2) 멀티스레드 상세 파싱(+이미지 저장)\n",
    "    books = parse_details_multithread(book_urls)\n",
    "\n",
    "    # 3) 저장 (image_path 포함)\n",
    "    out_path = DATA_DIR / \"books_5pages.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(books, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Data saved to {out_path}\")\n",
    "    print(f\"Images saved to {IMAGES_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea933227-7d11-49ba-8db4-9547ea634d0a",
   "metadata": {},
   "source": [
    "# duckdb에 데이터 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b9bfa-6d3f-4012-a964-279f69fa6b77",
   "metadata": {},
   "source": [
    "## Pandas DataFrame을 DuckDB 테이블 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d09694-48b7-426a-b540-9052a2a8235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product  total_sales\n",
      "0       A       6500.0\n",
      "1       B       7000.0\n",
      "2       C       3000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# 예시 Pandas DataFrame 생성\n",
    "df = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'price': [1000, 2000, 1500, 3000, 2500],\n",
    "    'quantity': [2, 1, 3, 1, 2]\n",
    "})\n",
    "\n",
    "# DuckDB에 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# DataFrame을 SQL 쿼리로 바로 사용\n",
    "# SQL 내에서 DataFrame 변수명(df)을 직접 사용\n",
    "result = con.execute(\"SELECT product, SUM(price * quantity) as total_sales FROM df GROUP BY product\").fetchdf()\n",
    "\n",
    "print(result)\n",
    "\n",
    "# 연결 닫기\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da209c-aa6a-433c-8527-5a942bb7a4ee",
   "metadata": {},
   "source": [
    "## DuckDB를 사용하여 파일 입출력 처리\n",
    "- DuckDB는 CSV, Parquet, JSON 등 다양한 파일 포맷을 직접 쿼리하고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde84c4a-17df-4793-a986-32dde9009c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame이 'iris_data.csv' 파일로 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Seaborn에서 'iris' 데이터셋 가져오기\n",
    "iris_df = sns.load_dataset('iris')\n",
    "\n",
    "# 2. DataFrame을 'iris_data.csv' 파일로 내보내기\n",
    "# index=False 옵션은 Pandas DataFrame의 인덱스를 파일에 포함시키지 않도록 합니다.\n",
    "iris_df.to_csv('data/iris_data.csv', index=False)\n",
    "print(\"DataFrame이 'iris_data.csv' 파일로 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a73cef9-4d3b-4946-a30e-f81f2521eb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DuckDB로 'iris_data.csv' 파일을 읽어온 결과:\n",
      "      species  avg_sepal_length\n",
      "0      setosa             5.006\n",
      "1  versicolor             5.936\n",
      "2   virginica             6.588\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# 1. DuckDB로 파일 가져와 쿼리하기\n",
    "# DuckDB에 인메모리(in-memory) 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# SQL 쿼리를 사용하여 CSV 파일에서 데이터 읽기\n",
    "# 'iris_data.csv' 파일을 마치 테이블처럼 직접 쿼리합니다.\n",
    "# species 별 sepal_length의 평균을 계산\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    species,\n",
    "    AVG(sepal_length) AS avg_sepal_length\n",
    "FROM 'data/iris_data.csv'\n",
    "GROUP BY species\n",
    "ORDER BY species;\n",
    "\"\"\"\n",
    "\n",
    "# 쿼리 실행 및 결과를 Pandas DataFrame으로 가져오기\n",
    "result_df = con.execute(query).fetchdf()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nDuckDB로 'iris_data.csv' 파일을 읽어온 결과:\")\n",
    "print(result_df)\n",
    "\n",
    "# DuckDB 연결 종료\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbbdfc-4066-47ee-906e-b35dd5fab7ea",
   "metadata": {},
   "source": [
    "## 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49189a90-7dbe-4fd0-a56a-62af580f626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas to_csv()로 'iris_to_csv_pandas.csv' 파일이 저장되었습니다.\n",
      "Pandas to_parquet()로 'iris_to_parquet_pandas.parquet' 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Seaborn에서 'iris' 데이터셋 불러오기\n",
    "iris_df = sns.load_dataset('iris')\n",
    "\n",
    "# CSV 파일로 저장\n",
    "iris_df.to_csv('data/iris_to_csv_pandas.csv', index=False)\n",
    "print(\"Pandas to_csv()로 'iris_to_csv_pandas.csv' 파일이 저장되었습니다.\")\n",
    "\n",
    "# Parquet 파일로 저장 (pyarrow 또는 fastparquet 엔진 필요)\n",
    "# pip install pyarrow\n",
    "iris_df.to_parquet('data/iris_to_parquet_pandas.parquet', index=False)\n",
    "print(\"Pandas to_parquet()로 'iris_to_parquet_pandas.parquet' 파일이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f36e1-0b36-4efc-8827-4289dfcf2f69",
   "metadata": {},
   "source": [
    "- CSV 파일 기반으로 COPY 명령어 사용하기\n",
    "- 기존에 Pandas로 저장했던 'iris_to_csv_pandas.csv' 파일을 읽어와서 특정 조건의 데이터를 필터링한 후, 새로운 CSV 파일로 다시 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00eeb436-382c-4982-996f-b2794abe4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'filtered_iris.csv' 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# 기존 CSV 파일 ('iris_to_csv_pandas.csv')을 읽어서 sepal_length가 5.5 이상인 데이터만 필터링\n",
    "# 결과를 'filtered_iris.csv' 파일로 저장\n",
    "query_from_csv = \"\"\"\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM 'data/iris_to_csv_pandas.csv'\n",
    "    WHERE \"sepal_length\" >= 5.5\n",
    ") TO 'data/filtered_iris.csv' (HEADER, DELIMITER ',');\n",
    "\"\"\"\n",
    "\n",
    "# 쿼리 실행\n",
    "con.execute(query_from_csv)\n",
    "print(\"CSV 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'filtered_iris.csv' 파일이 저장되었습니다.\")\n",
    "\n",
    "# 연결 종료\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772a570-823c-44f8-a957-bbdc1fb055df",
   "metadata": {},
   "source": [
    "- Parquet 파일 기반으로 COPY 명령어 사용하기\n",
    "- 기존에 Pandas로 저장했던 'data/iris_to_parquet_pandas.parquet' 파일을 읽어와서 species별로 sepal_width의 평균을 계산한 뒤, 그 결과를 Parquet 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9a1ddb-065d-43b8-9b6c-af23f4c1861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'avg_sepal_width.parquet' 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# 기존 Parquet 파일 ('iris_to_parquet_pandas.parquet')을 읽어서 species 별 평균 sepal_width 계산\n",
    "# 결과를 'avg_sepal_width.parquet' 파일로 저장\n",
    "query_from_parquet = \"\"\"\n",
    "COPY (\n",
    "    SELECT\n",
    "        species,\n",
    "        AVG(\"sepal_width\") AS avg_sepal_width\n",
    "    FROM 'data/iris_to_parquet_pandas.parquet'\n",
    "    GROUP BY species\n",
    ") TO 'data/avg_sepal_width.parquet' (FORMAT PARQUET);\n",
    "\"\"\"\n",
    "\n",
    "# 쿼리 실행\n",
    "con.execute(query_from_parquet)\n",
    "print(\"Parquet 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'avg_sepal_width.parquet' 파일이 저장되었습니다.\")\n",
    "\n",
    "# 연결 종료\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
