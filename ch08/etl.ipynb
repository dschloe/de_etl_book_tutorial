{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e590d45d-1ad5-480b-ab75-404e4035d35a",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69530bed-7b76-43ec-8d23-c94db601710f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAGE] http://books.toscrape.com/catalogue/page-1.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-2.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-3.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-4.html\n",
      "[PAGE] http://books.toscrape.com/catalogue/page-5.html\n",
      "[INFO] 수집된 책 URL 수: 100\n",
      "[PROGRESS] 1/100 완료 (1.0s 경과, ETA 96.7s)\n",
      "[PROGRESS] 2/100 완료 (1.1s 경과, ETA 51.6s)\n",
      "[PROGRESS] 3/100 완료 (1.3s 경과, ETA 41.2s)\n",
      "[PROGRESS] 4/100 완료 (1.7s 경과, ETA 40.0s)\n",
      "[PROGRESS] 5/100 완료 (1.9s 경과, ETA 35.4s)\n",
      "[PROGRESS] 6/100 완료 (2.0s 경과, ETA 31.8s)\n",
      "[PROGRESS] 7/100 완료 (2.0s 경과, ETA 26.9s)\n",
      "[PROGRESS] 8/100 완료 (2.2s 경과, ETA 25.2s)\n",
      "[PROGRESS] 9/100 완료 (2.2s 경과, ETA 22.2s)\n",
      "[PROGRESS] 10/100 완료 (2.3s 경과, ETA 20.8s)\n",
      "[PROGRESS] 11/100 완료 (2.4s 경과, ETA 19.0s)\n",
      "[PROGRESS] 12/100 완료 (2.4s 경과, ETA 17.3s)\n",
      "[PROGRESS] 13/100 완료 (2.4s 경과, ETA 16.0s)\n",
      "[PROGRESS] 14/100 완료 (2.5s 경과, ETA 15.1s)\n",
      "[PROGRESS] 15/100 완료 (2.5s 경과, ETA 14.1s)\n",
      "[PROGRESS] 16/100 완료 (2.6s 경과, ETA 13.5s)\n",
      "[PROGRESS] 17/100 완료 (2.8s 경과, ETA 13.8s)\n",
      "[PROGRESS] 18/100 완료 (2.8s 경과, ETA 12.9s)\n",
      "[PROGRESS] 19/100 완료 (2.8s 경과, ETA 12.1s)\n",
      "[PROGRESS] 20/100 완료 (3.0s 경과, ETA 11.9s)\n",
      "[PROGRESS] 21/100 완료 (3.0s 경과, ETA 11.3s)\n",
      "[PROGRESS] 22/100 완료 (3.0s 경과, ETA 10.7s)\n",
      "[PROGRESS] 23/100 완료 (3.2s 경과, ETA 10.7s)\n",
      "[PROGRESS] 24/100 완료 (3.4s 경과, ETA 10.7s)\n",
      "[PROGRESS] 25/100 완료 (3.4s 경과, ETA 10.2s)\n",
      "[PROGRESS] 26/100 완료 (3.5s 경과, ETA 9.9s)\n",
      "[PROGRESS] 27/100 완료 (3.5s 경과, ETA 9.5s)\n",
      "[PROGRESS] 28/100 완료 (3.5s 경과, ETA 9.1s)\n",
      "[PROGRESS] 29/100 완료 (3.7s 경과, ETA 9.1s)\n",
      "[PROGRESS] 30/100 완료 (3.8s 경과, ETA 8.8s)\n",
      "[PROGRESS] 31/100 완료 (3.8s 경과, ETA 8.5s)\n",
      "[PROGRESS] 32/100 완료 (3.8s 경과, ETA 8.1s)\n",
      "[PROGRESS] 33/100 완료 (4.0s 경과, ETA 8.1s)\n",
      "[PROGRESS] 34/100 완료 (4.1s 경과, ETA 7.9s)\n",
      "[PROGRESS] 35/100 완료 (4.1s 경과, ETA 7.6s)\n",
      "[PROGRESS] 36/100 완료 (4.1s 경과, ETA 7.4s)\n",
      "[PROGRESS] 37/100 완료 (4.4s 경과, ETA 7.5s)\n",
      "[PROGRESS] 38/100 완료 (4.4s 경과, ETA 7.2s)\n",
      "[PROGRESS] 39/100 완료 (4.7s 경과, ETA 7.4s)\n",
      "[PROGRESS] 40/100 완료 (4.7s 경과, ETA 7.1s)\n",
      "[PROGRESS] 41/100 완료 (4.7s 경과, ETA 6.8s)\n",
      "[PROGRESS] 42/100 완료 (4.8s 경과, ETA 6.7s)\n",
      "[PROGRESS] 43/100 완료 (4.9s 경과, ETA 6.5s)\n",
      "[PROGRESS] 44/100 완료 (4.9s 경과, ETA 6.2s)\n",
      "[PROGRESS] 45/100 완료 (5.2s 경과, ETA 6.3s)\n",
      "[PROGRESS] 46/100 완료 (5.2s 경과, ETA 6.1s)\n",
      "[PROGRESS] 47/100 완료 (5.2s 경과, ETA 5.9s)\n",
      "[PROGRESS] 48/100 완료 (5.3s 경과, ETA 5.7s)\n",
      "[PROGRESS] 49/100 완료 (5.3s 경과, ETA 5.5s)\n",
      "[PROGRESS] 50/100 완료 (5.4s 경과, ETA 5.4s)\n",
      "[PROGRESS] 51/100 완료 (5.4s 경과, ETA 5.2s)\n",
      "[PROGRESS] 52/100 완료 (5.5s 경과, ETA 5.1s)\n",
      "[PROGRESS] 53/100 완료 (5.6s 경과, ETA 5.0s)\n",
      "[PROGRESS] 54/100 완료 (5.8s 경과, ETA 5.0s)\n",
      "[PROGRESS] 55/100 완료 (5.9s 경과, ETA 4.8s)\n",
      "[PROGRESS] 56/100 완료 (6.1s 경과, ETA 4.8s)\n",
      "[PROGRESS] 57/100 완료 (6.1s 경과, ETA 4.6s)\n",
      "[PROGRESS] 58/100 완료 (6.1s 경과, ETA 4.4s)\n",
      "[PROGRESS] 59/100 완료 (6.1s 경과, ETA 4.2s)\n",
      "[PROGRESS] 60/100 완료 (6.2s 경과, ETA 4.1s)\n",
      "[PROGRESS] 61/100 완료 (6.3s 경과, ETA 4.0s)\n",
      "[PROGRESS] 62/100 완료 (6.3s 경과, ETA 3.9s)\n",
      "[PROGRESS] 63/100 완료 (6.3s 경과, ETA 3.7s)\n",
      "[PROGRESS] 64/100 완료 (6.5s 경과, ETA 3.6s)\n",
      "[PROGRESS] 65/100 완료 (6.5s 경과, ETA 3.5s)\n",
      "[PROGRESS] 66/100 완료 (6.7s 경과, ETA 3.4s)\n",
      "[PROGRESS] 67/100 완료 (6.7s 경과, ETA 3.3s)\n",
      "[PROGRESS] 68/100 완료 (6.9s 경과, ETA 3.3s)\n",
      "[PROGRESS] 69/100 완료 (6.9s 경과, ETA 3.1s)\n",
      "[PROGRESS] 70/100 완료 (7.0s 경과, ETA 3.0s)\n",
      "[PROGRESS] 71/100 완료 (7.1s 경과, ETA 2.9s)\n",
      "[PROGRESS] 72/100 완료 (7.2s 경과, ETA 2.8s)\n",
      "[PROGRESS] 73/100 완료 (7.2s 경과, ETA 2.7s)\n",
      "[PROGRESS] 74/100 완료 (7.4s 경과, ETA 2.6s)\n",
      "[PROGRESS] 75/100 완료 (7.4s 경과, ETA 2.5s)\n",
      "[PROGRESS] 76/100 완료 (7.4s 경과, ETA 2.3s)\n",
      "[PROGRESS] 77/100 완료 (7.4s 경과, ETA 2.2s)\n",
      "[PROGRESS] 78/100 완료 (7.5s 경과, ETA 2.1s)\n",
      "[PROGRESS] 79/100 완료 (7.5s 경과, ETA 2.0s)\n",
      "[PROGRESS] 80/100 완료 (7.6s 경과, ETA 1.9s)\n",
      "[PROGRESS] 81/100 완료 (7.7s 경과, ETA 1.8s)\n",
      "[PROGRESS] 82/100 완료 (7.8s 경과, ETA 1.7s)\n",
      "[PROGRESS] 83/100 완료 (7.8s 경과, ETA 1.6s)\n",
      "[PROGRESS] 84/100 완료 (8.0s 경과, ETA 1.5s)\n",
      "[PROGRESS] 85/100 완료 (8.0s 경과, ETA 1.4s)\n",
      "[PROGRESS] 86/100 완료 (8.1s 경과, ETA 1.3s)\n",
      "[PROGRESS] 87/100 완료 (8.1s 경과, ETA 1.2s)\n",
      "[PROGRESS] 88/100 완료 (8.2s 경과, ETA 1.1s)\n",
      "[PROGRESS] 89/100 완료 (8.2s 경과, ETA 1.0s)\n",
      "[PROGRESS] 90/100 완료 (8.4s 경과, ETA 0.9s)\n",
      "[PROGRESS] 91/100 완료 (8.5s 경과, ETA 0.8s)\n",
      "[PROGRESS] 92/100 완료 (8.5s 경과, ETA 0.7s)\n",
      "[PROGRESS] 93/100 완료 (8.6s 경과, ETA 0.6s)\n",
      "[PROGRESS] 94/100 완료 (8.6s 경과, ETA 0.5s)\n",
      "[PROGRESS] 95/100 완료 (8.6s 경과, ETA 0.5s)\n",
      "[PROGRESS] 96/100 완료 (8.7s 경과, ETA 0.4s)\n",
      "[PROGRESS] 97/100 완료 (8.8s 경과, ETA 0.3s)\n",
      "[PROGRESS] 98/100 완료 (8.8s 경과, ETA 0.2s)\n",
      "[PROGRESS] 99/100 완료 (8.9s 경과, ETA 0.1s)\n",
      "[PROGRESS] 100/100 완료 (9.1s 경과, ETA 0.0s)\n",
      "Data saved to data/books_5pages.json\n",
      "Images saved to data/images/\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Books to Scrape 크롤러 (멀티스레딩 / 5페이지 / 진행상황+ETA / 이미지 다운로드)\n",
    "- 상세 파싱과 동시에 표지 이미지를 data/images/ 에 저장\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import threading\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "START_URL = \"http://books.toscrape.com/catalogue/page-1.html\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "MAX_WORKERS = 16\n",
    "PER_REQUEST_PAUSE = 0.03\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "IMAGES_DIR = DATA_DIR / \"images\"\n",
    "\n",
    "print_lock = threading.Lock()\n",
    "\n",
    "def make_session():\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3, connect=3, read=3, backoff_factor=0.5,\n",
    "        status_forcelist=(429, 500, 502, 503, 504),\n",
    "        allowed_methods=frozenset([\"GET\"])\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    s.headers.update(HEADERS)\n",
    "    return s\n",
    "\n",
    "def clean_money(s):\n",
    "    return (s or \"\").replace(\"Â\", \"\").strip()\n",
    "\n",
    "def slugify(text, maxlen=80):\n",
    "    text = re.sub(r\"[^\\w\\s-]\", \"\", text, flags=re.UNICODE)  # 특수문자 제거\n",
    "    text = re.sub(r\"[\\s_-]+\", \"-\", text).strip(\"-\")         # 공백/언더스코어 → 하이픈\n",
    "    return text[:maxlen] if text else \"untitled\"\n",
    "\n",
    "def guess_ext_from_url(url, default=\"jpg\"):\n",
    "    path = urlparse(url).path\n",
    "    if \".\" in path:\n",
    "        ext = path.rsplit(\".\", 1)[-1].lower()\n",
    "        # 간단 화이트리스트\n",
    "        if ext in {\"jpg\", \"jpeg\", \"png\", \"webp\"}:\n",
    "            return \"jpg\" if ext == \"jpeg\" else ext\n",
    "    return default\n",
    "\n",
    "def download_image(sess, img_url, title, upc):\n",
    "    IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ext = guess_ext_from_url(img_url, default=\"jpg\")\n",
    "    name_part = (upc or slugify(title))\n",
    "    filename = f\"{name_part}.{ext}\"\n",
    "    dest = IMAGES_DIR / filename\n",
    "\n",
    "    # 동일 파일 존재 시 중복 방지(숫자 suffix)\n",
    "    if dest.exists():\n",
    "        i = 2\n",
    "        while True:\n",
    "            cand = IMAGES_DIR / f\"{name_part}-{i}.{ext}\"\n",
    "            if not cand.exists():\n",
    "                dest = cand\n",
    "                break\n",
    "            i += 1\n",
    "\n",
    "    try:\n",
    "        with sess.get(img_url, timeout=20, stream=True) as r:\n",
    "            r.raise_for_status()\n",
    "            with open(dest, \"wb\") as f:\n",
    "                for chunk in r.iter_content(chunk_size=8192):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        return str(dest)\n",
    "    except Exception as e:\n",
    "        with print_lock:\n",
    "            print(f\"(이미지 실패) {img_url} -> {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_book_detail(html, base_url):\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    details = {}\n",
    "    table = soup.find(\"table\", class_=\"table table-striped\")\n",
    "    if table:\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            th = row.find(\"th\").get_text(strip=True)\n",
    "            td = row.find(\"td\").get_text(strip=True)\n",
    "            details[th] = td\n",
    "\n",
    "    title = soup.find(\"div\", class_=\"product_main\").find(\"h1\").get_text(strip=True)\n",
    "    desc_anchor = soup.find(\"div\", id=\"product_description\")\n",
    "    description = desc_anchor.find_next_sibling(\"p\").get_text(strip=True) if desc_anchor else \"\"\n",
    "\n",
    "    img_tag = soup.select_one(\".item.active img\") or soup.find(\"img\")\n",
    "    img_rel = img_tag[\"src\"] if img_tag else \"\"\n",
    "    image_url = urljoin(base_url, img_rel)\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"upc\": details.get(\"UPC\"),\n",
    "        \"product_type\": details.get(\"Product Type\", \"\"),\n",
    "        \"price_excl_tax\": clean_money(details.get(\"Price (excl. tax)\")),\n",
    "        \"price_incl_tax\": clean_money(details.get(\"Price (incl. tax)\")),\n",
    "        \"tax\": clean_money(details.get(\"Tax\")),\n",
    "        \"availability\": details.get(\"Availability\", \"\"),\n",
    "        \"num_reviews\": details.get(\"Number of reviews\"),\n",
    "        \"description\": description,\n",
    "        \"image_url\": image_url,\n",
    "        \"url\": base_url,\n",
    "    }\n",
    "\n",
    "def get_book_details(url):\n",
    "    sess = make_session()\n",
    "    try:\n",
    "        resp = sess.get(url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        data = parse_book_detail(resp.text, url)\n",
    "\n",
    "        # 이미지 다운로드\n",
    "        img_saved_path = None\n",
    "        if data.get(\"image_url\"):\n",
    "            img_saved_path = download_image(\n",
    "                sess, data[\"image_url\"], data.get(\"title\", \"\"), data.get(\"upc\")\n",
    "            )\n",
    "        data[\"image_path\"] = img_saved_path\n",
    "\n",
    "        time.sleep(PER_REQUEST_PAUSE)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        with print_lock:\n",
    "            print(f\"(건너뜀) {url} -> {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        sess.close()\n",
    "\n",
    "def collect_book_urls(start_url, max_pages=5):\n",
    "    urls = []\n",
    "    page_url = start_url\n",
    "    page_count = 0\n",
    "    sess = make_session()\n",
    "\n",
    "    while page_url and page_count < max_pages:\n",
    "        with print_lock:\n",
    "            print(f\"[PAGE] {page_url}\")\n",
    "        resp = sess.get(page_url, timeout=15)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "        for art in soup.find_all(\"article\", class_=\"product_pod\"):\n",
    "            a = art.find(\"h3\").find(\"a\")\n",
    "            book_rel = a[\"href\"]\n",
    "            book_url = urljoin(page_url, book_rel)\n",
    "            urls.append(book_url)\n",
    "\n",
    "        page_count += 1\n",
    "        next_li = soup.find(\"li\", class_=\"next\")\n",
    "        page_url = urljoin(page_url, next_li.find(\"a\")[\"href\"]) if (next_li and page_count < max_pages) else None\n",
    "        time.sleep(PER_REQUEST_PAUSE)\n",
    "\n",
    "    sess.close()\n",
    "    return urls\n",
    "\n",
    "def parse_details_multithread(book_urls, max_workers=MAX_WORKERS):\n",
    "    results = []\n",
    "    total = len(book_urls)\n",
    "    done_cnt = 0\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
    "        fut_map = {ex.submit(get_book_details, u): u for u in book_urls}\n",
    "        for fut in as_completed(fut_map):\n",
    "            data = fut.result()\n",
    "            if data:\n",
    "                results.append(data)\n",
    "\n",
    "            done_cnt += 1\n",
    "            elapsed = time.perf_counter() - start_time\n",
    "            avg_time = elapsed / max(1, done_cnt)\n",
    "            eta = avg_time * (total - done_cnt)\n",
    "\n",
    "            with print_lock:\n",
    "                print(f\"[PROGRESS] {done_cnt}/{total} 완료 \"\n",
    "                      f\"({elapsed:.1f}s 경과, ETA {eta:.1f}s)\")\n",
    "\n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    IMAGES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # 1) URL 수집 (5페이지 한정)\n",
    "    book_urls = collect_book_urls(START_URL, max_pages=5)\n",
    "    print(f\"[INFO] 수집된 책 URL 수: {len(book_urls)}\")\n",
    "\n",
    "    # 2) 멀티스레드 상세 파싱(+이미지 저장)\n",
    "    books = parse_details_multithread(book_urls)\n",
    "\n",
    "    # 3) 저장 (image_path 포함)\n",
    "    out_path = DATA_DIR / \"books_5pages.json\"\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(books, f, ensure_ascii=False, indent=2)\n",
    "    print(f\"Data saved to {out_path}\")\n",
    "    print(f\"Images saved to {IMAGES_DIR}/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea933227-7d11-49ba-8db4-9547ea634d0a",
   "metadata": {},
   "source": [
    "# duckdb에 데이터 추가하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593b9bfa-6d3f-4012-a964-279f69fa6b77",
   "metadata": {},
   "source": [
    "## Pandas DataFrame을 DuckDB 테이블 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67d09694-48b7-426a-b540-9052a2a8235b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  product  total_sales\n",
      "0       C       3000.0\n",
      "1       A       6500.0\n",
      "2       B       7000.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import duckdb\n",
    "\n",
    "# 예시 Pandas DataFrame 생성\n",
    "df = pd.DataFrame({\n",
    "    'product': ['A', 'B', 'A', 'C', 'B'],\n",
    "    'price': [1000, 2000, 1500, 3000, 2500],\n",
    "    'quantity': [2, 1, 3, 1, 2]\n",
    "})\n",
    "\n",
    "# DuckDB에 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# DataFrame을 SQL 쿼리로 바로 사용\n",
    "# SQL 내에서 DataFrame 변수명(df)을 직접 사용\n",
    "result = con.execute(\"SELECT product, SUM(price * quantity) as total_sales FROM df GROUP BY product\").fetchdf()\n",
    "\n",
    "print(result)\n",
    "\n",
    "# 연결 닫기\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92da209c-aa6a-433c-8527-5a942bb7a4ee",
   "metadata": {},
   "source": [
    "## DuckDB를 사용하여 파일 입출력 처리\n",
    "- DuckDB는 CSV, Parquet, JSON 등 다양한 파일 포맷을 직접 쿼리하고 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bde84c4a-17df-4793-a986-32dde9009c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame이 'iris_data.csv' 파일로 성공적으로 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Seaborn에서 'iris' 데이터셋 가져오기\n",
    "iris_df = sns.load_dataset('iris')\n",
    "\n",
    "# 2. DataFrame을 'iris_data.csv' 파일로 내보내기\n",
    "# index=False 옵션은 Pandas DataFrame의 인덱스를 파일에 포함시키지 않도록 합니다.\n",
    "iris_df.to_csv('data/iris_data.csv', index=False)\n",
    "print(\"DataFrame이 'iris_data.csv' 파일로 성공적으로 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a73cef9-4d3b-4946-a30e-f81f2521eb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DuckDB로 'iris_data.csv' 파일을 읽어온 결과:\n",
      "      species  avg_sepal_length\n",
      "0      setosa             5.006\n",
      "1  versicolor             5.936\n",
      "2   virginica             6.588\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# 1. DuckDB로 파일 가져와 쿼리하기\n",
    "# DuckDB에 인메모리(in-memory) 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# SQL 쿼리를 사용하여 CSV 파일에서 데이터 읽기\n",
    "# 'iris_data.csv' 파일을 마치 테이블처럼 직접 쿼리합니다.\n",
    "# species 별 sepal_length의 평균을 계산\n",
    "query = \"\"\"\n",
    "SELECT\n",
    "    species,\n",
    "    AVG(sepal_length) AS avg_sepal_length\n",
    "FROM 'data/iris_data.csv'\n",
    "GROUP BY species\n",
    "ORDER BY species;\n",
    "\"\"\"\n",
    "\n",
    "# 쿼리 실행 및 결과를 Pandas DataFrame으로 가져오기\n",
    "result_df = con.execute(query).fetchdf()\n",
    "\n",
    "# 결과 출력\n",
    "print(\"\\nDuckDB로 'iris_data.csv' 파일을 읽어온 결과:\")\n",
    "print(result_df)\n",
    "\n",
    "# DuckDB 연결 종료\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fbbdfc-4066-47ee-906e-b35dd5fab7ea",
   "metadata": {},
   "source": [
    "## 파일 쓰기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49189a90-7dbe-4fd0-a56a-62af580f626f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pandas to_csv()로 'iris_to_csv_pandas.csv' 파일이 저장되었습니다.\n",
      "Pandas to_parquet()로 'iris_to_parquet_pandas.parquet' 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Seaborn에서 'iris' 데이터셋 불러오기\n",
    "iris_df = sns.load_dataset('iris')\n",
    "\n",
    "# CSV 파일로 저장\n",
    "iris_df.to_csv('data/iris_to_csv_pandas.csv', index=False)\n",
    "print(\"Pandas to_csv()로 'iris_to_csv_pandas.csv' 파일이 저장되었습니다.\")\n",
    "\n",
    "# Parquet 파일로 저장 (pyarrow 또는 fastparquet 엔진 필요)\n",
    "# pip install pyarrow\n",
    "iris_df.to_parquet('data/iris_to_parquet_pandas.parquet', index=False)\n",
    "print(\"Pandas to_parquet()로 'iris_to_parquet_pandas.parquet' 파일이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1f36e1-0b36-4efc-8827-4289dfcf2f69",
   "metadata": {},
   "source": [
    "- CSV 파일 기반으로 COPY 명령어 사용하기\n",
    "- 기존에 Pandas로 저장했던 'iris_to_csv_pandas.csv' 파일을 읽어와서 특정 조건의 데이터를 필터링한 후, 새로운 CSV 파일로 다시 저장한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00eeb436-382c-4982-996f-b2794abe4c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'filtered_iris.csv' 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# 기존 CSV 파일 ('iris_to_csv_pandas.csv')을 읽어서 sepal_length가 5.5 이상인 데이터만 필터링\n",
    "# 결과를 'filtered_iris.csv' 파일로 저장\n",
    "query_from_csv = \"\"\"\n",
    "COPY (\n",
    "    SELECT *\n",
    "    FROM 'data/iris_to_csv_pandas.csv'\n",
    "    WHERE \"sepal_length\" >= 5.5\n",
    ") TO 'data/filtered_iris.csv' (HEADER, DELIMITER ',');\n",
    "\"\"\"\n",
    "\n",
    "# 쿼리 실행\n",
    "con.execute(query_from_csv)\n",
    "print(\"CSV 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'filtered_iris.csv' 파일이 저장되었습니다.\")\n",
    "\n",
    "# 연결 종료\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8772a570-823c-44f8-a957-bbdc1fb055df",
   "metadata": {},
   "source": [
    "- Parquet 파일 기반으로 COPY 명령어 사용하기\n",
    "- 기존에 Pandas로 저장했던 'data/iris_to_parquet_pandas.parquet' 파일을 읽어와서 species별로 sepal_width의 평균을 계산한 뒤, 그 결과를 Parquet 파일로 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab9a1ddb-065d-43b8-9b6c-af23f4c1861e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parquet 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'avg_sepal_width.parquet' 파일이 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "# DuckDB 연결\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "# 기존 Parquet 파일 ('iris_to_parquet_pandas.parquet')을 읽어서 species 별 평균 sepal_width 계산\n",
    "# 결과를 'avg_sepal_width.parquet' 파일로 저장\n",
    "query_from_parquet = \"\"\"\n",
    "COPY (\n",
    "    SELECT\n",
    "        species,\n",
    "        AVG(\"sepal_width\") AS avg_sepal_width\n",
    "    FROM 'data/iris_to_parquet_pandas.parquet'\n",
    "    GROUP BY species\n",
    ") TO 'data/avg_sepal_width.parquet' (FORMAT PARQUET);\n",
    "\"\"\"\n",
    "\n",
    "# 쿼리 실행\n",
    "con.execute(query_from_parquet)\n",
    "print(\"Parquet 파일을 기반으로 DuckDB COPY 명령어가 실행되었습니다. 'avg_sepal_width.parquet' 파일이 저장되었습니다.\")\n",
    "\n",
    "# 연결 종료\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95599611-4f95-4d6c-ae00-a8b99c888de2",
   "metadata": {},
   "source": [
    "## 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd7ed88-636a-40ba-8e15-284e6d07da8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>avg_sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>2.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginica</td>\n",
       "      <td>2.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setosa</td>\n",
       "      <td>3.428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      species  avg_sepal_width\n",
       "0  versicolor            2.770\n",
       "1   virginica            2.974\n",
       "2      setosa            3.428"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parquet_file_path = \"data/avg_sepal_width.parquet\"\n",
    "\n",
    "pd.read_parquet(parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea95a97e-7b25-4c99-ae6f-e4a069acb24d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>avg_sepal_width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>versicolor</td>\n",
       "      <td>2.770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>virginica</td>\n",
       "      <td>2.974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>setosa</td>\n",
       "      <td>3.428</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      species  avg_sepal_width\n",
       "0  versicolor            2.770\n",
       "1   virginica            2.974\n",
       "2      setosa            3.428"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import duckdb\n",
    "\n",
    "query_to_read_parquet = \"\"\"\n",
    "SELECT * \n",
    "FROM 'data/avg_sepal_width.parquet';\n",
    "\"\"\"\n",
    "\n",
    "con = duckdb.connect(database=':memory:', read_only=False)\n",
    "\n",
    "con.execute(query_to_read_parquet).fetchdf()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99399b7-a6ce-410b-8ccd-26ce55b40a7a",
   "metadata": {},
   "source": [
    "# DuckDB에서 파일 저장된 테이블 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b64c9c73-d437-45fd-b59e-97fece21fe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DuckDB에서 데이터를 성공적으로 불러와 Pandas DataFrame으로 변환했습니다.\n",
      "DataFrame 정보:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 12 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   upc             100 non-null    object \n",
      " 1   title           100 non-null    object \n",
      " 2   product_type    100 non-null    object \n",
      " 3   price_excl_tax  100 non-null    float32\n",
      " 4   price_incl_tax  100 non-null    float32\n",
      " 5   tax             100 non-null    float32\n",
      " 6   availability    100 non-null    object \n",
      " 7   num_reviews     100 non-null    int32  \n",
      " 8   description     100 non-null    object \n",
      " 9   image_url       100 non-null    object \n",
      " 10  url             100 non-null    object \n",
      " 11  image_path      100 non-null    object \n",
      "dtypes: float32(3), int32(1), object(8)\n",
      "memory usage: 7.9+ KB\n",
      "\n",
      "상위 5개 데이터:\n",
      "                upc                                              title  \\\n",
      "0  90fa61229261140a                                 Tipping the Velvet   \n",
      "1  a897fe39b1053632                               A Light in the Attic   \n",
      "2  4165285e1663650f              Sapiens: A Brief History of Humankind   \n",
      "3  a34ba96d4081e6a4                          Rip it Up and Start Again   \n",
      "4  e10e1e165dc8be4a  The Boys in the Boat: Nine Americans and Their...   \n",
      "\n",
      "  product_type  price_excl_tax  price_incl_tax  tax             availability  \\\n",
      "0        Books       53.740002       53.740002  0.0  In stock (20 available)   \n",
      "1        Books       51.770000       51.770000  0.0  In stock (22 available)   \n",
      "2        Books       54.230000       54.230000  0.0  In stock (20 available)   \n",
      "3        Books       35.020000       35.020000  0.0  In stock (19 available)   \n",
      "4        Books       22.600000       22.600000  0.0  In stock (19 available)   \n",
      "\n",
      "   num_reviews                                        description  \\\n",
      "0            0  \"Erotic and absorbing...Written with starling ...   \n",
      "1            0  It's hard to imagine a world without A Light i...   \n",
      "2            0  From a renowned historian comes a groundbreaki...   \n",
      "3            0  Punk's raw power rejuvenated rock, but by the ...   \n",
      "4            0  For readers of Laura Hillenbrand's Seabiscuit ...   \n",
      "\n",
      "                                           image_url  \\\n",
      "0  http://books.toscrape.com/media/cache/08/e9/08...   \n",
      "1  http://books.toscrape.com/media/cache/fe/72/fe...   \n",
      "2  http://books.toscrape.com/media/cache/ce/5f/ce...   \n",
      "3  http://books.toscrape.com/media/cache/81/7f/81...   \n",
      "4  http://books.toscrape.com/media/cache/d1/2d/d1...   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://books.toscrape.com/catalogue/tipping-th...   \n",
      "1  http://books.toscrape.com/catalogue/a-light-in...   \n",
      "2  http://books.toscrape.com/catalogue/sapiens-a-...   \n",
      "3  http://books.toscrape.com/catalogue/rip-it-up-...   \n",
      "4  http://books.toscrape.com/catalogue/the-boys-i...   \n",
      "\n",
      "                         image_path  \n",
      "0  data/images/90fa61229261140a.jpg  \n",
      "1  data/images/a897fe39b1053632.jpg  \n",
      "2  data/images/4165285e1663650f.jpg  \n",
      "3  data/images/a34ba96d4081e6a4.jpg  \n",
      "4  data/images/e10e1e165dc8be4a.jpg  \n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# DuckDB 데이터베이스 파일 경로\n",
    "DB_PATH = Path(\"data/books_data.duckdb\")\n",
    "\n",
    "# 데이터베이스에 연결\n",
    "try:\n",
    "    con = duckdb.connect(database=str(DB_PATH), read_only=True)\n",
    "    \n",
    "    # SQL 쿼리를 실행하여 'books_clean' 테이블의 데이터를 DataFrame으로 불러오기\n",
    "    # DuckDB는 직접 DataFrame으로 결과를 반환하는 to_df() 메서드를 제공합니다.\n",
    "    books_df = con.sql(\"SELECT * FROM books_clean\").df()\n",
    "\n",
    "    # 데이터베이스 연결 종료\n",
    "    con.close()\n",
    "    \n",
    "    print(\"DuckDB에서 데이터를 성공적으로 불러와 Pandas DataFrame으로 변환했습니다.\")\n",
    "    print(\"DataFrame 정보:\")\n",
    "    print(type(books_df))\n",
    "    books_df.info()\n",
    "    \n",
    "    # 상위 5개 데이터 출력하여 확인\n",
    "    print(\"\\n상위 5개 데이터:\")\n",
    "    print(books_df.head())\n",
    "\n",
    "except duckdb.Error as e:\n",
    "    print(f\"DuckDB 연결 또는 쿼리 실행 중 오류가 발생했습니다: {e}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"오류: 데이터베이스 파일 '{DB_PATH}'을 찾을 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9332db-42fe-4ee6-a697-74e5f1a559b5",
   "metadata": {},
   "source": [
    "## DuckDB 테이블 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "60c52ec4-27a7-45fd-a1ba-b3d25ffb8871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data/books_data.duckdb' 파일에 연결하여 'books_clean' 테이블을 삭제합니다.\n",
      "'books_clean' 테이블이 성공적으로 삭제되었습니다.\n",
      "DuckDB 연결이 종료되었습니다.\n",
      "물리적 파일 'data/books_data.duckdb'을(를) 삭제합니다.\n",
      "'data/books_data.duckdb' 파일이 성공적으로 삭제되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# DuckDB 데이터베이스 파일 경로\n",
    "DB_PATH = Path(\"data/books_data.duckdb\")\n",
    "TABLE_NAME = \"books_clean\"\n",
    "\n",
    "def cleanup_duckdb():\n",
    "    \"\"\"\n",
    "    DuckDB 파일 내의 특정 테이블과 물리적 파일을 모두 삭제합니다.\n",
    "    \"\"\"\n",
    "    # 1. 파일 존재 여부 확인\n",
    "    if not DB_PATH.exists():\n",
    "        print(f\"파일 '{DB_PATH}'이(가) 존재하지 않습니다. 삭제할 항목이 없습니다.\")\n",
    "        return\n",
    "\n",
    "    # 2. DuckDB 파일에 연결하여 테이블 삭제\n",
    "    print(f\"'{DB_PATH}' 파일에 연결하여 '{TABLE_NAME}' 테이블을 삭제합니다.\")\n",
    "    con = None\n",
    "    try:\n",
    "        con = duckdb.connect(database=str(DB_PATH), read_only=False)\n",
    "        con.execute(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")\n",
    "        print(f\"'{TABLE_NAME}' 테이블이 성공적으로 삭제되었습니다.\")\n",
    "        \n",
    "    except duckdb.Error as e:\n",
    "        print(f\"테이블 삭제 중 오류가 발생했습니다: {e}\")\n",
    "    finally:\n",
    "        if con:\n",
    "            con.close()\n",
    "            print(\"DuckDB 연결이 종료되었습니다.\")\n",
    "\n",
    "    # 3. 물리적 파일 삭제\n",
    "    print(f\"물리적 파일 '{DB_PATH}'을(를) 삭제합니다.\")\n",
    "    try:\n",
    "        os.remove(DB_PATH)\n",
    "        print(f\"'{DB_PATH}' 파일이 성공적으로 삭제되었습니다.\")\n",
    "    except OSError as e:\n",
    "        print(f\"물리적 파일 삭제 중 오류가 발생했습니다: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cleanup_duckdb()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
