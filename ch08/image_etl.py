# -*- coding: utf-8 -*-
"""
Books to Scrape í¬ë¡¤ëŸ¬ (ë©€í‹°ìŠ¤ë ˆë”© / 5í˜ì´ì§€ / ì§„í–‰ìƒí™©+ETA / ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ)
- ìƒì„¸ íŒŒì‹±ê³¼ ë™ì‹œì— í‘œì§€ ì´ë¯¸ì§€ë¥¼ data/images/ ì— ì €ì¥
- ë‹¤ìš´ë¡œë“œëœ ì´ë¯¸ì§€ì˜ ì„ë² ë”©ì„ ìƒì„±í•˜ì—¬ Milvusì— ì €ì¥
"""

import os
import re
import json
import time
import threading
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from urllib.parse import urljoin, urlparse

import requests
from bs4 import BeautifulSoup
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

from pymilvus import MilvusClient, DataType
from sentence_transformers import SentenceTransformer
from PIL import Image

START_URL = "http://books.toscrape.com/catalogue/page-1.html"
HEADERS = {"User-Agent": "Mozilla/5.0"}
MAX_WORKERS = 16
PER_REQUEST_PAUSE = 0.03

DATA_DIR = Path("data")
IMAGES_DIR = DATA_DIR / "images"

print_lock = threading.Lock()

# --- Milvus ë° ëª¨ë¸ ì„¤ì • ---
MILVUS_HOST = os.environ.get("MILVUS_HOST", "localhost")
MILVUS_PORT = os.environ.get("MILVUS_PORT", "19530")
MILVUS_COLLECTION_NAME = "book_images_collection"

# ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
try:
    # Milvus ë¬¸ì„œì—ì„œ ê¶Œì¥í•˜ëŠ” ëª¨ë¸ ì¤‘ í•˜ë‚˜ì¸ 'clip-ViT-B-32'ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì´ ëª¨ë¸ì€ ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ë¥¼ ê°™ì€ ë²¡í„° ê³µê°„ì— ì„ë² ë”©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # í•œê¸€ ì²˜ë¦¬ë¥¼ ìœ„í•´ 'multilingual-v1' ë²„ì „ìœ¼ë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
    image_model = SentenceTransformer("clip-ViT-B-32-multilingual-v1")
    print("âœ… ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì™„ë£Œ.")
except Exception as e:
    print(f"âŒ ì´ë¯¸ì§€ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    image_model = None

def make_session():
    """ìš”ì²­ ì„¸ì…˜ì„ ìƒì„±í•˜ê³  ì¬ì‹œë„ ì •ì±…ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
    s = requests.Session()
    retries = Retry(
        total=3, connect=3, read=3, backoff_factor=0.5,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=frozenset(["GET"])
    )
    adapter = HTTPAdapter(max_retries=retries, pool_connections=100, pool_maxsize=100)
    s.mount("http://", adapter)
    s.mount("https://", adapter)
    s.headers.update(HEADERS)
    return s

def clean_money(s):
    """í†µí™” ê¸°í˜¸ì™€ ë¶ˆí•„ìš”í•œ ë¬¸ìë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
    return (s or "").replace("Ã‚", "").strip()

def slugify(text, maxlen=80):
    """íŒŒì¼ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í…ìŠ¤íŠ¸ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤."""
    text = re.sub(r"[^\w\s-]", "", text, flags=re.UNICODE)
    text = re.sub(r"[\s_-]+", "-", text).strip("-")
    return text[:maxlen] if text else "untitled"

def guess_ext_from_url(url, default="jpg"):
    """URLì—ì„œ ì´ë¯¸ì§€ í™•ì¥ìë¥¼ ì¶”ì¸¡í•©ë‹ˆë‹¤."""
    path = urlparse(url).path
    if "." in path:
        ext = path.rsplit(".", 1)[-1].lower()
        if ext in {"jpg", "jpeg", "png", "webp"}:
            return "jpg" if ext == "jpeg" else ext
    return default

def download_image(sess, img_url, title, upc):
    """ì±… ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ë¡œì»¬ ê²½ë¡œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)
    ext = guess_ext_from_url(img_url, default="jpg")
    name_part = (upc or slugify(title))
    filename = f"{name_part}.{ext}"
    dest = IMAGES_DIR / filename

    # íŒŒì¼ëª… ì¤‘ë³µ ë°©ì§€
    if dest.exists():
        i = 2
        while True:
            cand = IMAGES_DIR / f"{name_part}-{i}.{ext}"
            if not cand.exists():
                dest = cand
                break
            i += 1

    try:
        with sess.get(img_url, timeout=20, stream=True) as r:
            r.raise_for_status()
            with open(dest, "wb") as f:
                for chunk in r.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
        return str(dest)
    except Exception as e:
        with print_lock:
            print(f"(ì´ë¯¸ì§€ ì‹¤íŒ¨) {img_url} -> {e}")
        return None

def parse_book_detail(html, base_url):
    """ì±… ìƒì„¸ í˜ì´ì§€ HTMLì„ íŒŒì‹±í•˜ì—¬ ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    soup = BeautifulSoup(html, "html.parser")

    details = {}
    table = soup.find("table", class_="table table-striped")
    if table:
        for row in table.find_all("tr"):
            th = row.find("th").get_text(strip=True)
            td = row.find("td").get_text(strip=True)
            details[th] = td

    title = soup.find("div", class_="product_main").find("h1").get_text(strip=True)
    desc_anchor = soup.find("div", id="product_description")
    description = desc_anchor.find_next_sibling("p").get_text(strip=True) if desc_anchor else ""

    img_tag = soup.select_one(".item.active img") or soup.find("img")
    img_rel = img_tag["src"] if img_tag else ""
    image_url = urljoin(base_url, img_rel)

    return {
        "title": title,
        "upc": details.get("UPC"),
        "product_type": details.get("Product Type", ""),
        "price_excl_tax": clean_money(details.get("Price (excl. tax)")),
        "price_incl_tax": clean_money(details.get("Price (incl. tax)")),
        "tax": clean_money(details.get("Tax")),
        "availability": details.get("Availability", ""),
        "num_reviews": details.get("Number of reviews"),
        "description": description,
        "image_url": image_url,
        "url": base_url,
    }

def get_book_details(url):
    """ì±… ìƒì„¸ ì •ë³´ì™€ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."""
    sess = make_session()
    try:
        resp = sess.get(url, timeout=15)
        resp.raise_for_status()
        data = parse_book_detail(resp.text, url)

        img_saved_path = None
        if data.get("image_url"):
            img_saved_path = download_image(
                sess, data["image_url"], data.get("title", ""), data.get("upc")
            )
        data["image_path"] = img_saved_path

        time.sleep(PER_REQUEST_PAUSE)
        return data
    except Exception as e:
        with print_lock:
            print(f"(ê±´ë„ˆëœ€) {url} -> {e}")
        return None
    finally:
        sess.close()

def collect_book_urls(start_url, max_pages=5):
    """ì±… ëª©ë¡ í˜ì´ì§€ë¥¼ ìˆœíšŒí•˜ë©° ìƒì„¸ í˜ì´ì§€ URLì„ ìˆ˜ì§‘í•©ë‹ˆë‹¤."""
    urls = []
    page_url = start_url
    page_count = 0
    sess = make_session()

    while page_url and page_count < max_pages:
        with print_lock:
            print(f"[PAGE] {page_url}")
        try:
            resp = sess.get(page_url, timeout=15)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.text, "html.parser")

            for art in soup.find_all("article", class_="product_pod"):
                a = art.find("h3").find("a")
                book_rel = a["href"]
                book_url = urljoin(page_url, book_rel)
                urls.append(book_url)

            page_count += 1
            next_li = soup.find("li", class_="next")
            page_url = urljoin(page_url, next_li.find("a")["href"]) if (next_li and page_count < max_pages) else None
            time.sleep(PER_REQUEST_PAUSE)
        except Exception as e:
            with print_lock:
                print(f"(í˜ì´ì§€ ê±´ë„ˆëœ€) {page_url} -> {e}")
            break

    sess.close()
    return urls

def parse_details_multithread(book_urls, milvus_client, max_workers=MAX_WORKERS):
    """ë©€í‹°ìŠ¤ë ˆë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì±… ìƒì„¸ í˜ì´ì§€ë¥¼ íŒŒì‹±í•˜ê³  Milvusì— ë°ì´í„°ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤."""
    results = []
    total = len(book_urls)
    done_cnt = 0
    start_time = time.perf_counter()

    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        fut_map = {ex.submit(get_book_details, u): u for u in book_urls}
        for fut in as_completed(fut_map):
            data = fut.result()
            if data:
                if data.get("image_path") and image_model:
                    try:
                        # ì´ë¯¸ì§€ íŒŒì¼ ì—´ê¸°
                        image = Image.open(data["image_path"])
                        # ì´ë¯¸ì§€ ì„ë² ë”© ìƒì„± (ë²¡í„°í™”)
                        image_embedding = image_model.encode(image)
                        
                        # Milvusì— ë°ì´í„° ì‚½ì…
                        milvus_client.insert(
                            collection_name=MILVUS_COLLECTION_NAME,
                            data=[{
                                "vector": image_embedding.tolist(),
                                "title": data.get("title", "N/A"),
                                "url": data.get("url", "N/A"),
                                "image_url": data.get("image_url", "N/A"),
                                "upc": data.get("upc", "N/A"),
                            }]
                        )
                        
                        with print_lock:
                            print(f"âœ… Milvusì— '{data['title']}' ì´ë¯¸ì§€ ë²¡í„° ì¶”ê°€ ì™„ë£Œ.")

                    except Exception as e:
                        with print_lock:
                            print(f"âŒ '{data.get('title', 'N/A')}' ì´ë¯¸ì§€ ì²˜ë¦¬ ë˜ëŠ” Milvus ì‚½ì… ì‹¤íŒ¨: {e}")

                results.append(data)

            done_cnt += 1
            elapsed = time.perf_counter() - start_time
            avg_time = elapsed / max(1, done_cnt)
            eta = avg_time * (total - done_cnt)

            with print_lock:
                print(f"[PROGRESS] {done_cnt}/{total} ì™„ë£Œ "
                      f"({elapsed:.1f}s ê²½ê³¼, ETA {eta:.1f}s)")

    return results

def create_milvus_collection(milvus_client):
    """Milvus ì»¬ë ‰ì…˜ì„ ìƒì„±í•˜ê±°ë‚˜ ì¬ìƒì„±í•©ë‹ˆë‹¤."""
    if milvus_client.has_collection(collection_name=MILVUS_COLLECTION_NAME):
        milvus_client.drop_collection(collection_name=MILVUS_COLLECTION_NAME)
        print(f"ğŸ—‘ï¸ ê¸°ì¡´ '{MILVUS_COLLECTION_NAME}' ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ.")

    # `clip-ViT-B-32-multilingual-v1` ëª¨ë¸ì˜ ì„ë² ë”© ì°¨ì›ì€ 512ì…ë‹ˆë‹¤.
    milvus_client.create_collection(
        collection_name=MILVUS_COLLECTION_NAME,
        dimension=512,
        primary_field_name="pk",
        vector_field_name="vector",
        auto_id=True,
        schema=[
            {"name": "pk", "type": DataType.INT64, "is_primary": True, "auto_id": True},
            {"name": "vector", "type": DataType.FLOAT_VECTOR, "dim": 512},
            {"name": "title", "type": DataType.VARCHAR, "max_length": 256},
            {"name": "url", "type": DataType.VARCHAR, "max_length": 512},
            {"name": "image_url", "type": DataType.VARCHAR, "max_length": 512},
            {"name": "upc", "type": DataType.VARCHAR, "max_length": 64},
        ],
        index_params={
            "field_name": "vector",
            "metric_type": "COSINE", # ì´ë¯¸ì§€ ìœ ì‚¬ë„ ê²€ìƒ‰ì— ì í•©í•œ 'ì½”ì‚¬ì¸' ìœ ì‚¬ë„ ì‚¬ìš©
            "index_type": "IVF_FLAT",
            "params": {"nlist": 128}
        }
    )
    print(f"âœ¨ '{MILVUS_COLLECTION_NAME}' ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ.")


if __name__ == "__main__":
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    IMAGES_DIR.mkdir(parents=True, exist_ok=True)

    # Milvus í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
    try:
        milvus_client = MilvusClient(
            uri=f"http://{MILVUS_HOST}:{MILVUS_PORT}",
            db_name="default",
            timeout=10
        )
        print(f"ğŸ”— Milvus í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì„±ê³µ: {MILVUS_HOST}:{MILVUS_PORT}")
    except Exception as e:
        print(f"âŒ Milvus í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì‹¤íŒ¨: {e}")
        milvus_client = None

    if milvus_client:
        create_milvus_collection(milvus_client)

        # 1) URL ìˆ˜ì§‘ (5í˜ì´ì§€ í•œì •)
        book_urls = collect_book_urls(START_URL, max_pages=5)
        print(f"[INFO] ìˆ˜ì§‘ëœ ì±… URL ìˆ˜: {len(book_urls)}")

        # 2) ë©€í‹°ìŠ¤ë ˆë“œ ìƒì„¸ íŒŒì‹±(+ì´ë¯¸ì§€ ì €ì¥ & Milvus ì €ì¥)
        books = parse_details_multithread(book_urls, milvus_client)

        milvus_client.close()

    # 3) íŒŒì¼ ì €ì¥ (ê¸°ì¡´ê³¼ ë™ì¼)
    out_path = DATA_DIR / "books_5pages.json"
    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(books, f, ensure_ascii=False, indent=2)
    print(f"Data saved to {out_path}")
    print(f"Images saved to {IMAGES_DIR}/")