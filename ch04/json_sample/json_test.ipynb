{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cfc45b-366b-4fa6-ac81-394cd22bc91f",
   "metadata": {},
   "source": [
    "# 라이브러리 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f21f839-6891-49c6-aeb7-6bdb3dbb9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc # 가비지 컬렉션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b20ab-e1d3-41eb-ac54-9944f1ad42b7",
   "metadata": {},
   "source": [
    "# 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6984e9d0-4181-4c82-b3f6-7441e3447b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'large_data.jsonl' 파일 생성 중... (상당한 시간 소요 예상)\n",
      "  100000 객체 작성 완료...\n",
      "  200000 객체 작성 완료...\n",
      "  300000 객체 작성 완료...\n",
      "  400000 객체 작성 완료...\n",
      "  500000 객체 작성 완료...\n",
      "  600000 객체 작성 완료...\n",
      "  700000 객체 작성 완료...\n",
      "  800000 객체 작성 완료...\n",
      "  900000 객체 작성 완료...\n",
      "  1000000 객체 작성 완료...\n",
      "  1100000 객체 작성 완료...\n",
      "  1200000 객체 작성 완료...\n",
      "  1300000 객체 작성 완료...\n",
      "  1400000 객체 작성 완료...\n",
      "  1500000 객체 작성 완료...\n",
      "  1600000 객체 작성 완료...\n",
      "  1700000 객체 작성 완료...\n",
      "  1800000 객체 작성 완료...\n",
      "  1900000 객체 작성 완료...\n",
      "  2000000 객체 작성 완료...\n",
      "  2100000 객체 작성 완료...\n",
      "  2200000 객체 작성 완료...\n",
      "  2300000 객체 작성 완료...\n",
      "  2400000 객체 작성 완료...\n",
      "  2500000 객체 작성 완료...\n",
      "  2600000 객체 작성 완료...\n",
      "  2700000 객체 작성 완료...\n",
      "  2800000 객체 작성 완료...\n",
      "  2900000 객체 작성 완료...\n",
      "  3000000 객체 작성 완료...\n",
      "  3100000 객체 작성 완료...\n",
      "  3200000 객체 작성 완료...\n",
      "  3300000 객체 작성 완료...\n",
      "  3400000 객체 작성 완료...\n",
      "  3500000 객체 작성 완료...\n",
      "  3600000 객체 작성 완료...\n",
      "  3700000 객체 작성 완료...\n",
      "  3800000 객체 작성 완료...\n",
      "  3900000 객체 작성 완료...\n",
      "  4000000 객체 작성 완료...\n",
      "  4100000 객체 작성 완료...\n",
      "  4200000 객체 작성 완료...\n",
      "  4300000 객체 작성 완료...\n",
      "  4400000 객체 작성 완료...\n",
      "  4500000 객체 작성 완료...\n",
      "  4600000 객체 작성 완료...\n",
      "  4700000 객체 작성 완료...\n",
      "  4800000 객체 작성 완료...\n",
      "  4900000 객체 작성 완료...\n",
      "  5000000 객체 작성 완료...\n",
      "'large_data.jsonl' 파일 생성 완료. 소요 시간: 33.83 초\n",
      "생성된 'large_data.jsonl' 파일 크기: 1069.71 MB\n",
      "  약 1.04 GB\n"
     ]
    }
   ],
   "source": [
    "# --- 1. 대용량 JSONL 데이터 생성 ---\n",
    "jsonl_file_name = \"large_data.jsonl\" # JSON Lines 파일명\n",
    "num_json_objects = 5_000_000 # 약 50만 개의 JSON 객체 (대략 1GB 예상)\n",
    "num_columns = 10 # 각 객체의 컬럼 수\n",
    "\n",
    "def generate_single_json_object(obj_id, num_cols):\n",
    "    \"\"\"단일 JSON 객체 생성 함수\"\"\"\n",
    "    return {f\"col_{j}\": f\"value_{obj_id}_{j}\" if j % 2 == 0 else random.randint(1, 1_000_000)\n",
    "            for j in range(num_cols)}\n",
    "\n",
    "if not os.path.exists(jsonl_file_name):\n",
    "    print(f\"'{jsonl_file_name}' 파일 생성 중... (상당한 시간 소요 예상)\")\n",
    "    start_gen = time.time()\n",
    "    \n",
    "    with open(jsonl_file_name, 'w', encoding='utf-8') as f:\n",
    "        for i in range(num_json_objects):\n",
    "            # 각 객체를 JSON 문자열로 변환하고 줄바꿈 추가\n",
    "            f.write(json.dumps(generate_single_json_object(i, num_columns)) + '\\n')\n",
    "            if (i + 1) % 100_000 == 0: # 10만 객체마다 진행 상황 출력\n",
    "                print(f\"  {i + 1} 객체 작성 완료...\")\n",
    "        \n",
    "    end_gen = time.time()\n",
    "    print(f\"'{jsonl_file_name}' 파일 생성 완료. 소요 시간: {end_gen - start_gen:.2f} 초\")\n",
    "    gc.collect() \n",
    "else:\n",
    "    print(f\"'{jsonl_file_name}' 파일이 이미 존재합니다. 다시 생성하지 않습니다.\")\n",
    "\n",
    "# 생성된 파일 크기 확인\n",
    "def get_file_size_mb_gb(file_path):\n",
    "    if not os.path.exists(file_path): return 0\n",
    "    size_in_bytes = os.path.getsize(file_path)\n",
    "    return size_in_bytes / (1024 * 1024) # MB 단위로 반환\n",
    "\n",
    "file_size_mb = get_file_size_mb_gb(jsonl_file_name)\n",
    "if file_size_mb > 0:\n",
    "    print(f\"생성된 '{jsonl_file_name}' 파일 크기: {file_size_mb:.2f} MB\")\n",
    "    if file_size_mb > 1024:\n",
    "        print(f\"  약 {file_size_mb / 1024:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795bc3d0-89ae-4858-95d4-394c4f05eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. JSONL 추출 속도 측정 함수들 (일부 수정) ---\n",
    "\n",
    "# (2-1) json 모듈 (단일 스레드)\n",
    "def read_jsonl_sequential(file_path):\n",
    "    \"\"\"\n",
    "    Python의 내장 `json` 모듈로 JSONL 파일을 순차적으로 줄 단위로 읽습니다.\n",
    "    반환된 리스트를 Pandas DataFrame으로 변환합니다.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # 빈 줄 건너뛰기\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# (2-2) json 모듈 + 멀티프로세싱\n",
    "def process_jsonl_chunk(chunk_lines):\n",
    "    \"\"\"JSONL 줄 청크를 받아 각 줄을 JSON 객체로 파싱\"\"\"\n",
    "    processed_objects = []\n",
    "    for line in chunk_lines:\n",
    "        if line.strip():\n",
    "            processed_objects.append(json.loads(line))\n",
    "    return processed_objects\n",
    "\n",
    "def read_jsonl_parallel_multiprocessing(file_path, chunk_size_lines=50000):\n",
    "    \"\"\"\n",
    "    JSONL 파일을 청크 단위로 나눠 멀티프로세싱으로 파싱합니다.\n",
    "    파일을 줄 단위로 읽으면서 청크를 만들고, 이 청크를 병렬 파싱합니다.\n",
    "    반환된 리스트를 Pandas DataFrame으로 변환합니다.\n",
    "    \"\"\"\n",
    "    all_data_async_results = []\n",
    "    num_processes = cpu_count()\n",
    "    pool = Pool(num_processes)\n",
    "\n",
    "    temp_lines = []\n",
    "    line_count = 0\n",
    "    print(f\"  [MP] '{file_path}' 파일 읽으며 청크 분배 중...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            temp_lines.append(line)\n",
    "            line_count += 1\n",
    "            if line_count % chunk_size_lines == 0:\n",
    "                all_data_async_results.append(pool.apply_async(process_jsonl_chunk, (temp_lines,)))\n",
    "                temp_lines = []\n",
    "        if temp_lines: # 마지막 남은 청크 처리\n",
    "            all_data_async_results.append(pool.apply_async(process_jsonl_chunk, (temp_lines,)))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(f\"  [MP] 모든 청크 병렬 처리 완료. 결과 병합 중...\")\n",
    "    final_processed_data = []\n",
    "    for result_obj in all_data_async_results:\n",
    "        final_processed_data.extend(result_obj.get())\n",
    "\n",
    "    return final_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e817b2e-7abb-4804-b1c7-984681784225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- JSONL 추출 속도 비교 시작 ---\n",
      "\n",
      "--- json 모듈 (단일 스레드) - JSONL 추출 및 DataFrame 변환 시작 ---\n",
      "json (단일 스레드) JSONL 추출 및 DataFrame 변환 완료. 총 5000000 줄, 소요 시간: 50.04 초\n",
      "\n",
      "--- 멀티프로세싱 (json 모듈) - JSONL 추출 및 DataFrame 변환 시작 ---\n",
      "  [MP] 'large_data.jsonl' 파일 읽으며 청크 분배 중...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. 각 방식별 시간 측정 및 메모리 관리 ---\n",
    "\n",
    "# 결과를 저장할 변수 초기화\n",
    "jsonl_sequential_time = 0\n",
    "jsonl_multiprocessing_time = 0\n",
    "pandas_jsonl_time = 0\n",
    "polars_jsonl_time = 0\n",
    "jsonl_file_name = \"large_data.jsonl\" # JSON Lines 파일명\n",
    "\n",
    "if os.path.exists(jsonl_file_name):\n",
    "    print(\"\\n--- JSONL 추출 속도 비교 시작 ---\")\n",
    "\n",
    "    # 🔸 (3-1) json 모듈 (단일 스레드) -> Pandas DataFrame\n",
    "    print(\"\\n--- json 모듈 (단일 스레드) - JSONL 추출 및 DataFrame 변환 시작 ---\")\n",
    "    start_time = time.time()\n",
    "    extracted_data_jsonl_seq = read_jsonl_sequential(jsonl_file_name)\n",
    "    df_jsonl_seq = pd.DataFrame(extracted_data_jsonl_seq) # 리스트를 DataFrame으로 변환\n",
    "    end_time = time.time()\n",
    "    jsonl_sequential_time = end_time - start_time\n",
    "    print(f\"json (단일 스레드) JSONL 추출 및 DataFrame 변환 완료. 총 {len(df_jsonl_seq)} 줄, 소요 시간: {jsonl_sequential_time:.2f} 초\")\n",
    "    del extracted_data_jsonl_seq\n",
    "    del df_jsonl_seq\n",
    "    gc.collect()\n",
    "\n",
    "    # 🔸 (3-2) json 모듈 + 멀티프로세싱 -> Pandas DataFrame\n",
    "    print(\"\\n--- 멀티프로세싱 (json 모듈) - JSONL 추출 및 DataFrame 변환 시작 ---\")\n",
    "    start_time = time.time()\n",
    "    extracted_data_jsonl_mp = read_jsonl_parallel_multiprocessing(jsonl_file_name)\n",
    "    df_jsonl_mp = pd.DataFrame(extracted_data_jsonl_mp) # 리스트를 DataFrame으로 변환\n",
    "    end_time = time.time()\n",
    "    jsonl_multiprocessing_time = end_time - start_time\n",
    "    print(f\"멀티프로세싱 (json 모듈) JSONL 추출 및 DataFrame 변환 완료. 총 {len(df_jsonl_mp)} 줄, 소요 시간: {jsonl_multiprocessing_time:.2f} 초\")\n",
    "    del extracted_data_jsonl_mp\n",
    "    del df_jsonl_mp\n",
    "    gc.collect()\n",
    "\n",
    "    # 🔸 (3-3) Pandas 활용 (lines=True)\n",
    "    print(\"\\n--- Pandas를 이용한 JSONL 추출 시작 (lines=True) ---\")\n",
    "    start_time = time.time()\n",
    "    df_pandas_jsonl = pd.read_json(jsonl_file_name, lines=True)\n",
    "    end_time = time.time()\n",
    "    pandas_jsonl_time = end_time - start_time\n",
    "    print(f\"Pandas JSONL 추출 완료. 총 {len(df_pandas_jsonl)} 줄, 소요 시간: {pandas_jsonl_time:.2f} 초\")\n",
    "    del df_pandas_jsonl\n",
    "    gc.collect()\n",
    "\n",
    "    # 🔸 (3-4) Polars 활용 -> Pandas DataFrame (Polars는 자체적으로 DataFrame 반환, Pandas로 변환 추가)\n",
    "    print(\"\\n--- Polars를 이용한 JSONL 추출 시작 ---\")\n",
    "    start_time = time.time()\n",
    "    df_polars_jsonl_pl = pl.read_ndjson(jsonl_file_name) # Polars의 read_json은 기본적으로 JSONL에 최적화\n",
    "    df_polars_jsonl = df_polars_jsonl_pl.to_pandas() # Polars DataFrame을 Pandas DataFrame으로 변환\n",
    "    end_time = time.time()\n",
    "    polars_jsonl_time = end_time - start_time\n",
    "    print(f\"Polars JSONL 추출 완료. 총 {len(df_polars_jsonl)} 줄, 소요 시간: {polars_jsonl_time:.2f} 초\")\n",
    "    del df_polars_jsonl_pl\n",
    "    del df_polars_jsonl\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(f\"'{jsonl_file_name}' 파일이 없습니다. 먼저 파일을 생성해주세요.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b48b2-ca01-4010-8afe-d3767419eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. JSONL 추출 속도 비교 시각화 ---\n",
    "labels_jsonl = ['json (Seq) to DF', 'multiprocessing (json) to DF', 'Pandas read_json', 'Polars read_json to DF']\n",
    "times_jsonl = [jsonl_sequential_time, jsonl_multiprocessing_time, pandas_jsonl_time, polars_jsonl_time]\n",
    "\n",
    "# 모든 시간이 유효한지 확인\n",
    "if all(time_val > 0 for time_val in times_jsonl):\n",
    "    fig, ax = plt.subplots(figsize=(12, 7)) # Adjust figure size for better label readability\n",
    "    bars = ax.bar(labels_jsonl, times_jsonl, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "\n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('JSONL Data to Pandas DataFrame Extraction Time Comparison (Lower is Better)')\n",
    "    ax.set_ylim(0, max(times_jsonl) * 1.2) # y축 범위 설정\n",
    "\n",
    "    # 막대 위에 시간 값 표시\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.1, f'{yval:.2f} sec', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nJSONL 추출 시간 데이터가 완전하지 않아 그래프를 그릴 수 없습니다.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
