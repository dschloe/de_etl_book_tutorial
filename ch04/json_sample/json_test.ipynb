{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1cfc45b-366b-4fa6-ac81-394cd22bc91f",
   "metadata": {},
   "source": [
    "# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f21f839-6891-49c6-aeb7-6bdb3dbb9063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gc # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3b20ab-e1d3-41eb-ac54-9944f1ad42b7",
   "metadata": {},
   "source": [
    "# ë°ì´í„° ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6984e9d0-4181-4c82-b3f6-7441e3447b6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'large_data.jsonl' íŒŒì¼ ìƒì„± ì¤‘... (ìƒë‹¹í•œ ì‹œê°„ ì†Œìš” ì˜ˆìƒ)\n",
      "  100000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  200000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  300000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  400000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  500000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  600000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  700000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  800000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  900000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1000000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1100000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1200000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1300000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1400000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1500000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1600000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1700000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1800000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  1900000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2000000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2100000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2200000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2300000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2400000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2500000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2600000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2700000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2800000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  2900000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3000000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3100000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3200000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3300000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3400000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3500000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3600000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3700000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3800000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  3900000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4000000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4100000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4200000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4300000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4400000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4500000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4600000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4700000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4800000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  4900000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "  5000000 ê°ì²´ ì‘ì„± ì™„ë£Œ...\n",
      "'large_data.jsonl' íŒŒì¼ ìƒì„± ì™„ë£Œ. ì†Œìš” ì‹œê°„: 33.83 ì´ˆ\n",
      "ìƒì„±ëœ 'large_data.jsonl' íŒŒì¼ í¬ê¸°: 1069.71 MB\n",
      "  ì•½ 1.04 GB\n"
     ]
    }
   ],
   "source": [
    "# --- 1. ëŒ€ìš©ëŸ‰ JSONL ë°ì´í„° ìƒì„± ---\n",
    "jsonl_file_name = \"large_data.jsonl\" # JSON Lines íŒŒì¼ëª…\n",
    "num_json_objects = 5_000_000 # ì•½ 50ë§Œ ê°œì˜ JSON ê°ì²´ (ëŒ€ëµ 1GB ì˜ˆìƒ)\n",
    "num_columns = 10 # ê° ê°ì²´ì˜ ì»¬ëŸ¼ ìˆ˜\n",
    "\n",
    "def generate_single_json_object(obj_id, num_cols):\n",
    "    \"\"\"ë‹¨ì¼ JSON ê°ì²´ ìƒì„± í•¨ìˆ˜\"\"\"\n",
    "    return {f\"col_{j}\": f\"value_{obj_id}_{j}\" if j % 2 == 0 else random.randint(1, 1_000_000)\n",
    "            for j in range(num_cols)}\n",
    "\n",
    "if not os.path.exists(jsonl_file_name):\n",
    "    print(f\"'{jsonl_file_name}' íŒŒì¼ ìƒì„± ì¤‘... (ìƒë‹¹í•œ ì‹œê°„ ì†Œìš” ì˜ˆìƒ)\")\n",
    "    start_gen = time.time()\n",
    "    \n",
    "    with open(jsonl_file_name, 'w', encoding='utf-8') as f:\n",
    "        for i in range(num_json_objects):\n",
    "            # ê° ê°ì²´ë¥¼ JSON ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ê³  ì¤„ë°”ê¿ˆ ì¶”ê°€\n",
    "            f.write(json.dumps(generate_single_json_object(i, num_columns)) + '\\n')\n",
    "            if (i + 1) % 100_000 == 0: # 10ë§Œ ê°ì²´ë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "                print(f\"  {i + 1} ê°ì²´ ì‘ì„± ì™„ë£Œ...\")\n",
    "        \n",
    "    end_gen = time.time()\n",
    "    print(f\"'{jsonl_file_name}' íŒŒì¼ ìƒì„± ì™„ë£Œ. ì†Œìš” ì‹œê°„: {end_gen - start_gen:.2f} ì´ˆ\")\n",
    "    gc.collect() \n",
    "else:\n",
    "    print(f\"'{jsonl_file_name}' íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¤ì‹œ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "# ìƒì„±ëœ íŒŒì¼ í¬ê¸° í™•ì¸\n",
    "def get_file_size_mb_gb(file_path):\n",
    "    if not os.path.exists(file_path): return 0\n",
    "    size_in_bytes = os.path.getsize(file_path)\n",
    "    return size_in_bytes / (1024 * 1024) # MB ë‹¨ìœ„ë¡œ ë°˜í™˜\n",
    "\n",
    "file_size_mb = get_file_size_mb_gb(jsonl_file_name)\n",
    "if file_size_mb > 0:\n",
    "    print(f\"ìƒì„±ëœ '{jsonl_file_name}' íŒŒì¼ í¬ê¸°: {file_size_mb:.2f} MB\")\n",
    "    if file_size_mb > 1024:\n",
    "        print(f\"  ì•½ {file_size_mb / 1024:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "795bc3d0-89ae-4858-95d4-394c4f05eaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. JSONL ì¶”ì¶œ ì†ë„ ì¸¡ì • í•¨ìˆ˜ë“¤ (ì¼ë¶€ ìˆ˜ì •) ---\n",
    "\n",
    "# (2-1) json ëª¨ë“ˆ (ë‹¨ì¼ ìŠ¤ë ˆë“œ)\n",
    "def read_jsonl_sequential(file_path):\n",
    "    \"\"\"\n",
    "    Pythonì˜ ë‚´ì¥ `json` ëª¨ë“ˆë¡œ JSONL íŒŒì¼ì„ ìˆœì°¨ì ìœ¼ë¡œ ì¤„ ë‹¨ìœ„ë¡œ ì½ìŠµë‹ˆë‹¤.\n",
    "    ë°˜í™˜ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            if line.strip(): # ë¹ˆ ì¤„ ê±´ë„ˆë›°ê¸°\n",
    "                data.append(json.loads(line))\n",
    "    return data\n",
    "\n",
    "# (2-2) json ëª¨ë“ˆ + ë©€í‹°í”„ë¡œì„¸ì‹±\n",
    "def process_jsonl_chunk(chunk_lines):\n",
    "    \"\"\"JSONL ì¤„ ì²­í¬ë¥¼ ë°›ì•„ ê° ì¤„ì„ JSON ê°ì²´ë¡œ íŒŒì‹±\"\"\"\n",
    "    processed_objects = []\n",
    "    for line in chunk_lines:\n",
    "        if line.strip():\n",
    "            processed_objects.append(json.loads(line))\n",
    "    return processed_objects\n",
    "\n",
    "def read_jsonl_parallel_multiprocessing(file_path, chunk_size_lines=50000):\n",
    "    \"\"\"\n",
    "    JSONL íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ë‚˜ëˆ  ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    "    íŒŒì¼ì„ ì¤„ ë‹¨ìœ„ë¡œ ì½ìœ¼ë©´ì„œ ì²­í¬ë¥¼ ë§Œë“¤ê³ , ì´ ì²­í¬ë¥¼ ë³‘ë ¬ íŒŒì‹±í•©ë‹ˆë‹¤.\n",
    "    ë°˜í™˜ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ Pandas DataFrameìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    all_data_async_results = []\n",
    "    num_processes = cpu_count()\n",
    "    pool = Pool(num_processes)\n",
    "\n",
    "    temp_lines = []\n",
    "    line_count = 0\n",
    "    print(f\"  [MP] '{file_path}' íŒŒì¼ ì½ìœ¼ë©° ì²­í¬ ë¶„ë°° ì¤‘...\")\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            temp_lines.append(line)\n",
    "            line_count += 1\n",
    "            if line_count % chunk_size_lines == 0:\n",
    "                all_data_async_results.append(pool.apply_async(process_jsonl_chunk, (temp_lines,)))\n",
    "                temp_lines = []\n",
    "        if temp_lines: # ë§ˆì§€ë§‰ ë‚¨ì€ ì²­í¬ ì²˜ë¦¬\n",
    "            all_data_async_results.append(pool.apply_async(process_jsonl_chunk, (temp_lines,)))\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    print(f\"  [MP] ëª¨ë“  ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ. ê²°ê³¼ ë³‘í•© ì¤‘...\")\n",
    "    final_processed_data = []\n",
    "    for result_obj in all_data_async_results:\n",
    "        final_processed_data.extend(result_obj.get())\n",
    "\n",
    "    return final_processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e817b2e-7abb-4804-b1c7-984681784225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- JSONL ì¶”ì¶œ ì†ë„ ë¹„êµ ì‹œì‘ ---\n",
      "\n",
      "--- json ëª¨ë“ˆ (ë‹¨ì¼ ìŠ¤ë ˆë“œ) - JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì‹œì‘ ---\n",
      "json (ë‹¨ì¼ ìŠ¤ë ˆë“œ) JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì™„ë£Œ. ì´ 5000000 ì¤„, ì†Œìš” ì‹œê°„: 50.04 ì´ˆ\n",
      "\n",
      "--- ë©€í‹°í”„ë¡œì„¸ì‹± (json ëª¨ë“ˆ) - JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì‹œì‘ ---\n",
      "  [MP] 'large_data.jsonl' íŒŒì¼ ì½ìœ¼ë©° ì²­í¬ ë¶„ë°° ì¤‘...\n"
     ]
    }
   ],
   "source": [
    "# --- 3. ê° ë°©ì‹ë³„ ì‹œê°„ ì¸¡ì • ë° ë©”ëª¨ë¦¬ ê´€ë¦¬ ---\n",
    "\n",
    "# ê²°ê³¼ë¥¼ ì €ì¥í•  ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "jsonl_sequential_time = 0\n",
    "jsonl_multiprocessing_time = 0\n",
    "pandas_jsonl_time = 0\n",
    "polars_jsonl_time = 0\n",
    "jsonl_file_name = \"large_data.jsonl\" # JSON Lines íŒŒì¼ëª…\n",
    "\n",
    "if os.path.exists(jsonl_file_name):\n",
    "    print(\"\\n--- JSONL ì¶”ì¶œ ì†ë„ ë¹„êµ ì‹œì‘ ---\")\n",
    "\n",
    "    # ğŸ”¸ (3-1) json ëª¨ë“ˆ (ë‹¨ì¼ ìŠ¤ë ˆë“œ) -> Pandas DataFrame\n",
    "    print(\"\\n--- json ëª¨ë“ˆ (ë‹¨ì¼ ìŠ¤ë ˆë“œ) - JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì‹œì‘ ---\")\n",
    "    start_time = time.time()\n",
    "    extracted_data_jsonl_seq = read_jsonl_sequential(jsonl_file_name)\n",
    "    df_jsonl_seq = pd.DataFrame(extracted_data_jsonl_seq) # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    end_time = time.time()\n",
    "    jsonl_sequential_time = end_time - start_time\n",
    "    print(f\"json (ë‹¨ì¼ ìŠ¤ë ˆë“œ) JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì™„ë£Œ. ì´ {len(df_jsonl_seq)} ì¤„, ì†Œìš” ì‹œê°„: {jsonl_sequential_time:.2f} ì´ˆ\")\n",
    "    del extracted_data_jsonl_seq\n",
    "    del df_jsonl_seq\n",
    "    gc.collect()\n",
    "\n",
    "    # ğŸ”¸ (3-2) json ëª¨ë“ˆ + ë©€í‹°í”„ë¡œì„¸ì‹± -> Pandas DataFrame\n",
    "    print(\"\\n--- ë©€í‹°í”„ë¡œì„¸ì‹± (json ëª¨ë“ˆ) - JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì‹œì‘ ---\")\n",
    "    start_time = time.time()\n",
    "    extracted_data_jsonl_mp = read_jsonl_parallel_multiprocessing(jsonl_file_name)\n",
    "    df_jsonl_mp = pd.DataFrame(extracted_data_jsonl_mp) # ë¦¬ìŠ¤íŠ¸ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    end_time = time.time()\n",
    "    jsonl_multiprocessing_time = end_time - start_time\n",
    "    print(f\"ë©€í‹°í”„ë¡œì„¸ì‹± (json ëª¨ë“ˆ) JSONL ì¶”ì¶œ ë° DataFrame ë³€í™˜ ì™„ë£Œ. ì´ {len(df_jsonl_mp)} ì¤„, ì†Œìš” ì‹œê°„: {jsonl_multiprocessing_time:.2f} ì´ˆ\")\n",
    "    del extracted_data_jsonl_mp\n",
    "    del df_jsonl_mp\n",
    "    gc.collect()\n",
    "\n",
    "    # ğŸ”¸ (3-3) Pandas í™œìš© (lines=True)\n",
    "    print(\"\\n--- Pandasë¥¼ ì´ìš©í•œ JSONL ì¶”ì¶œ ì‹œì‘ (lines=True) ---\")\n",
    "    start_time = time.time()\n",
    "    df_pandas_jsonl = pd.read_json(jsonl_file_name, lines=True)\n",
    "    end_time = time.time()\n",
    "    pandas_jsonl_time = end_time - start_time\n",
    "    print(f\"Pandas JSONL ì¶”ì¶œ ì™„ë£Œ. ì´ {len(df_pandas_jsonl)} ì¤„, ì†Œìš” ì‹œê°„: {pandas_jsonl_time:.2f} ì´ˆ\")\n",
    "    del df_pandas_jsonl\n",
    "    gc.collect()\n",
    "\n",
    "    # ğŸ”¸ (3-4) Polars í™œìš© -> Pandas DataFrame (PolarsëŠ” ìì²´ì ìœ¼ë¡œ DataFrame ë°˜í™˜, Pandasë¡œ ë³€í™˜ ì¶”ê°€)\n",
    "    print(\"\\n--- Polarsë¥¼ ì´ìš©í•œ JSONL ì¶”ì¶œ ì‹œì‘ ---\")\n",
    "    start_time = time.time()\n",
    "    df_polars_jsonl_pl = pl.read_ndjson(jsonl_file_name) # Polarsì˜ read_jsonì€ ê¸°ë³¸ì ìœ¼ë¡œ JSONLì— ìµœì í™”\n",
    "    df_polars_jsonl = df_polars_jsonl_pl.to_pandas() # Polars DataFrameì„ Pandas DataFrameìœ¼ë¡œ ë³€í™˜\n",
    "    end_time = time.time()\n",
    "    polars_jsonl_time = end_time - start_time\n",
    "    print(f\"Polars JSONL ì¶”ì¶œ ì™„ë£Œ. ì´ {len(df_polars_jsonl)} ì¤„, ì†Œìš” ì‹œê°„: {polars_jsonl_time:.2f} ì´ˆ\")\n",
    "    del df_polars_jsonl_pl\n",
    "    del df_polars_jsonl\n",
    "    gc.collect()\n",
    "\n",
    "else:\n",
    "    print(f\"'{jsonl_file_name}' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € íŒŒì¼ì„ ìƒì„±í•´ì£¼ì„¸ìš”.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b48b2-ca01-4010-8afe-d3767419eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. JSONL ì¶”ì¶œ ì†ë„ ë¹„êµ ì‹œê°í™” ---\n",
    "labels_jsonl = ['json (Seq) to DF', 'multiprocessing (json) to DF', 'Pandas read_json', 'Polars read_json to DF']\n",
    "times_jsonl = [jsonl_sequential_time, jsonl_multiprocessing_time, pandas_jsonl_time, polars_jsonl_time]\n",
    "\n",
    "# ëª¨ë“  ì‹œê°„ì´ ìœ íš¨í•œì§€ í™•ì¸\n",
    "if all(time_val > 0 for time_val in times_jsonl):\n",
    "    fig, ax = plt.subplots(figsize=(12, 7)) # Adjust figure size for better label readability\n",
    "    bars = ax.bar(labels_jsonl, times_jsonl, color=['skyblue', 'lightcoral', 'lightgreen', 'gold'])\n",
    "\n",
    "    ax.set_ylabel('Time (seconds)')\n",
    "    ax.set_title('JSONL Data to Pandas DataFrame Extraction Time Comparison (Lower is Better)')\n",
    "    ax.set_ylim(0, max(times_jsonl) * 1.2) # yì¶• ë²”ìœ„ ì„¤ì •\n",
    "\n",
    "    # ë§‰ëŒ€ ìœ„ì— ì‹œê°„ ê°’ í‘œì‹œ\n",
    "    for bar in bars:\n",
    "        yval = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, yval + 0.1, f'{yval:.2f} sec', ha='center', va='bottom')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nJSONL ì¶”ì¶œ ì‹œê°„ ë°ì´í„°ê°€ ì™„ì „í•˜ì§€ ì•Šì•„ ê·¸ë˜í”„ë¥¼ ê·¸ë¦´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
